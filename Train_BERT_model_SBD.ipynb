{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mathijsgeelen/Sentence-extraction-annotation/blob/main/Train_BERT_model_SBD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required installations"
      ],
      "metadata": {
        "id": "B6zsHeEfHuRS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayAw4lOX9FgW",
        "outputId": "9ee4d274-ced9-459d-f259-df7b359ed5d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 5.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 37.5 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 93 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.14.1\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ewKEgyl9JY4",
        "outputId": "aa692467-931d-42f4-a97e-a5497cec9be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvnCWyZD9Kh8",
        "outputId": "e5914741-3029-4883-da08-eb38271e65b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzZSjaS-9Lq5",
        "outputId": "d6b1e97d-17eb-4603-99b3-7991b8abf30b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=61ae1d977a9123e19285387291d16ef929adc6e26e7828e177d2c953cd7ac15e\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVRYovyOMnJf",
        "outputId": "644e19d2-77a4-419c-9a7b-08e91c5b4182"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.9-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.1-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 64.0 MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=7221b2d7d26d0c1bcd4dd53dbe1e20105b861414647ba63154a6e5787f9f1c62\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=78664cf3d125a3ba347b1ddaf4f0a71ed076eba863beb56dea1ee059483f7028\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.24 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.1 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.9 yaspin-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required imports"
      ],
      "metadata": {
        "id": "kJaRhIgGHybq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "moYME-UJ9Mn4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import AutoModelForTokenClassification, AdamW\n",
        "import random\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
        "from transformers import DebertaTokenizer, DebertaForTokenClassification\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IcuRmYm09PZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa95ab5-61ab-4b2a-c476-75d03d452401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "tf.random.set_seed(seed)\n",
        "TF_DETERMINISTIC_OPS=seed\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate desired Transformer model\n",
        "Change the below model and tokenizer if you want to train any other model than DeBERTa"
      ],
      "metadata": {
        "id": "8M4hkrzyLq25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "model = DebertaForTokenClassification.from_pretrained(\"microsoft/deberta-base\", num_labels=4,output_attentions=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRLuKwsiLoeb",
        "outputId": "ea1c5ee7-6a27-486b-c7af-ab6bc140ea49"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForTokenClassification: ['lm_predictions.lm_head.bias', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n",
            "- This IS expected if you are initializing DebertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount google drive containing the 4 file of Savelka et al. (2017).\n",
        "Savelka, Jaromir, Vern R. Walker, Matthias Grabmair and Kevin D. Ashley. \"Sentence Boundary Detection in Adjudicatory Decisions in the Uniter States.\" TAL 58.2 (2017)."
      ],
      "metadata": {
        "id": "FgR7ywg-H3rT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB4Jnj_c9QZT",
        "outputId": "fd7f1fb4-570d-4dcf-e428-42e14ced92b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save all the files in your google drive in a folder named SBD, or change the file paths in the below imports"
      ],
      "metadata": {
        "id": "U1kZi--3IYqb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "26AOnWCJ9S6w"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"/content/drive/My Drive/SBD/bva.json.txt\") as file:\n",
        "  data_bva = json.load(file)\n",
        "with open(\"/content/drive/My Drive/SBD/cyber_crime.json.txt\") as file:\n",
        "  data_cyber = json.load(file)\n",
        "with open(\"/content/drive/My Drive/SBD/intellectual_property.json.txt\") as file:\n",
        "  data_ip = json.load(file)\n",
        "with open(\"/content/drive/My Drive/SBD/scotus.json.txt\") as file:\n",
        "  data_scotus = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information on dataset"
      ],
      "metadata": {
        "id": "5jXbEacPIkbl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38xpjs8Fu1ic",
        "outputId": "1afa4045-c1ef-4e50-dbdd-9b2051de2f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: BVA; Smallest sentence: 1, Longest sentence: 134, Average sentence: 20.8\n",
            "\n",
            "Dataset: Cyber; Smallest sentence: 1, Longest sentence: 276, Average sentence: 19.8\n",
            "\n",
            "Dataset: IP; Smallest sentence: 1, Longest sentence: 334, Average sentence: 21.2\n",
            "\n",
            "Dataset: Scotus; Smallest sentence: 1, Longest sentence: 614, Average sentence: 24.1\n",
            "\n",
            "Average sentence lenght of entire dataset: 21.5\n"
          ]
        }
      ],
      "source": [
        "def extract_info():\n",
        "  ### Extracts information on the lenght of the sentences in the dataset\n",
        "  names = [\"BVA\",\"Cyber\",\"IP\",\"Scotus\"]\n",
        "  av = []\n",
        "  for name, data in enumerate([data_bva, data_cyber, data_ip, data_scotus]):\n",
        "    low = 10000\n",
        "    high = 0\n",
        "    average = []\n",
        "    for counter, doc in enumerate(data):\n",
        "      document = data[doc]\n",
        "      for i in range(len(document[\"annotations\"])):\n",
        "        start = document[\"annotations\"][i][\"start\"]\n",
        "        end = document[\"annotations\"][i][\"end\"]\n",
        "        sent = document[\"text\"][start:end+1]\n",
        "        len_sent = len(sent.split(\" \"))\n",
        "        if len_sent < low:\n",
        "          low = len_sent\n",
        "        if len_sent > high:\n",
        "          high = len_sent\n",
        "        average.append(len_sent)\n",
        "        av.append(len_sent)\n",
        "    print(\"Dataset: {}; Smallest sentence: {}, Longest sentence: {}, Average sentence: {:.1f}\".format(names[name],low,high, np.mean(average)))\n",
        "    print()\n",
        "  print(\"Average sentence lenght of entire dataset: {:.1f}\".format(np.mean(av)))\n",
        "\n",
        "extract_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjCrh1xCt_lE",
        "outputId": "29d3d9b2-41e3-4b23-c2ce-e24e09ad0d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of sentences that end with punctuation mark in BVA: 0.850\n",
            "Percentage of sentences that end with punctuation mark in Cyber: 0.721\n",
            "Percentage of sentences that end with punctuation mark in IP: 0.760\n",
            "Percentage of sentences that end with punctuation mark in Scotus: 0.734\n",
            "Percentage of sentences that end with punctuation mark in total dataset: 0.754\n"
          ]
        }
      ],
      "source": [
        "def ends_with_punctuation():\n",
        "  ### extracts information on the number of sentences and characters in the dataset\n",
        "  total_count = 0\n",
        "  total_sentences = 0\n",
        "  names = [\"BVA\",\"Cyber\",\"IP\",\"Scotus\"]\n",
        "  for name, data in enumerate([data_bva, data_cyber, data_ip, data_scotus]):\n",
        "    count = 0\n",
        "    sentences = 0\n",
        "    for counter, doc in enumerate(data):\n",
        "      document = data[doc]\n",
        "      sentences += len(document[\"annotations\"])\n",
        "      total_sentences += len(document[\"annotations\"])\n",
        "      for i in range(len(document[\"annotations\"])):\n",
        "        end = document[\"annotations\"][i][\"end\"]\n",
        "        token = document[\"text\"][end-1]\n",
        "        if token == \".\" or token == \"?\" or token == \"!\":\n",
        "          count+=1\n",
        "          total_count += 1\n",
        "        \n",
        "    print(\"Percentage of sentences that end with punctuation mark in {}: {:.3f}\".format(names[name],count/sentences))\n",
        "  print(\"Percentage of sentences that end with punctuation mark in total dataset: {:.3f}\".format(total_count/total_sentences))\n",
        "\n",
        "\n",
        "ends_with_punctuation()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions to create the dataset"
      ],
      "metadata": {
        "id": "vbWgDU42Is41"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py0p_CuB9Zq0",
        "outputId": "367e2605-93f7-45e3-ee59-46e5781484fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete dataset contains 25899 sentences\n",
            "This function should return 25899\n"
          ]
        }
      ],
      "source": [
        "def create_sentences():\n",
        "  #create dataset consisting of the individual documents with accompanying sentences split using the annotations.\n",
        "  sentences = []\n",
        "  tags = []\n",
        "  for data in [data_bva, data_cyber, data_ip, data_scotus]:\n",
        "    for counter, doc in enumerate(data):\n",
        "      document = data[doc]\n",
        "      for i in range(len(document[\"annotations\"])):\n",
        "        start = document[\"annotations\"][i][\"start\"]\n",
        "        end = document[\"annotations\"][i][\"end\"]\n",
        "\n",
        "        prev_start = document[\"annotations\"][i-1][\"start\"]\n",
        "        prev_end = document[\"annotations\"][i-1][\"end\"]\n",
        "        sent = document[\"text\"][start:end+1]\n",
        "        prev_sent = document[\"text\"][prev_start:prev_end+1]\n",
        "        if prev_sent[-1] == \" \":\n",
        "          sent = \" \" + sent\n",
        "        if sent[-1] == \" \":\n",
        "          sent = sent[:-1]\n",
        "        sentences.append(sent)\n",
        "\n",
        "  return sentences\n",
        "\n",
        "print(\"Complete dataset contains {} sentences\".format(len(create_sentences())))\n",
        "\n",
        "print(\"This function should return 25899\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "I0uJl5OWLhc5"
      },
      "outputs": [],
      "source": [
        "def concatenate_sentences(extracted_sentences):\n",
        "  ### this function loops over the individual sentences and concatenates them to create batches on lenght 512 tokens\n",
        "  MAX_LEN = 510\n",
        "  mas_sequence_list = []\n",
        "  mas_label_list = []\n",
        "  ct_sequence_length = 0\n",
        "  ct_seq_list = []\n",
        "  ct_token_list = []\n",
        "  original_sent = []\n",
        "  tokenized_sent = []\n",
        "  for i, ind_sent in enumerate(extracted_sentences):\n",
        "    # add each sentence to a new sequence of size 510 tokens + 2 required tokens\n",
        "    sent = tokenizer.tokenize(ind_sent)\n",
        "    original_sent.append(ind_sent)\n",
        "    tokenized_sent.append(sent)\n",
        "    \n",
        "    ct_sent_length = len(sent)\n",
        "    if ct_sent_length <= 1:\n",
        "      continue\n",
        "    if (ct_sequence_length + ct_sent_length) < MAX_LEN:\n",
        "      # add the current sentence to current sequence\n",
        "      for item in sent:\n",
        "        ct_seq_list.append (item)\n",
        "      # label 2 indicates begining of a sentence\n",
        "      ct_token_list.append(2)\n",
        "      # label 1 is inside a sentence\n",
        "      for j in range(ct_sent_length - 2):\n",
        "        ct_token_list.append(1)\n",
        "      # label 3 is end of a sentence\n",
        "      ct_token_list.append(3)\n",
        "      ct_sequence_length = ct_sequence_length + ct_sent_length \n",
        "\n",
        "      # for debugging - can remove now\n",
        "      if (len(ct_token_list) != len(ct_seq_list)):\n",
        "        print ('mistmatch in length', i)\n",
        "        print (ct_seq_list)\n",
        "        print (ct_token_list)\n",
        "        print ('labels ', len(ct_token_list))\n",
        "        print ('word-tokens ', len(ct_seq_list))\n",
        "  #    ct_sequence_length + ct_sent_length more than max_length. split sentence\n",
        "  # add one part to current sequence, another to next sequence\n",
        "    else:\n",
        "      # the split sentence is more than 510\n",
        "      num_left_in_ct_seq = MAX_LEN - ct_sequence_length\n",
        "      first_part_ct_sent = sent[ :num_left_in_ct_seq]\n",
        "      last_part_ct_sent = sent [ num_left_in_ct_seq : ]\n",
        "\n",
        "      # following very special case - a second split is needed\n",
        "\n",
        "\n",
        "      # add first part to ct_seq_list\n",
        "      for item2 in first_part_ct_sent:\n",
        "        ct_seq_list.append (item2)\n",
        "      ct_token_list.append(2)\n",
        "      for k in range(num_left_in_ct_seq - 1):\n",
        "        ct_token_list.append(1)\n",
        "\n",
        "\n",
        "        ## deal with exceptions last part is null\n",
        "      if len(last_part_ct_sent) == 0:\n",
        "        ct_token_list[-1] = 3\n",
        "        #print ('zero len', len(ct_seq_list))\n",
        "        mas_sequence_list.append(ct_seq_list)\n",
        "        mas_label_list.append(ct_token_list)\n",
        "        ct_seq_list = []\n",
        "        ct_token_list = []\n",
        "        ct_sequence_length = 0\n",
        "      elif len(last_part_ct_sent) == 1:\n",
        "        #print ('one len', len(ct_seq_list))\n",
        "        mas_sequence_list.append(ct_seq_list)\n",
        "        mas_label_list.append(ct_token_list)\n",
        "        ct_seq_list = []\n",
        "        ct_token_list = []\n",
        "        for item4 in last_part_ct_sent:\n",
        "          ct_seq_list.append (item4)\n",
        "        ct_token_list.append(3)\n",
        "        ct_sequence_length = 1\n",
        "      else:\n",
        "        #print ('normal len', len(ct_seq_list))\n",
        "        mas_sequence_list.append(ct_seq_list)\n",
        "        mas_label_list.append(ct_token_list)\n",
        "        ct_seq_list = []\n",
        "        ct_token_list = []\n",
        "        if len(last_part_ct_sent) < MAX_LEN:\n",
        "          for item3 in last_part_ct_sent:\n",
        "            ct_seq_list.append (item3)\n",
        "            ct_token_list.append(1)\n",
        "          ct_token_list[-1] = 3\n",
        "          ct_sequence_length = len(last_part_ct_sent)\n",
        "        else:     # need to split again.\n",
        "          ## there are 3 sentences that require multiple splits\n",
        "          ## iterate   len (last_part_ct_sent) / MAX_LEN\n",
        "          num_iter = len(last_part_ct_sent) // MAX_LEN\n",
        "          last_segment = len(last_part_ct_sent) % MAX_LEN\n",
        "          #print('split sentence iter  last_seg ', num_iter, last_segment)\n",
        "          # iterate num_iter times to split large into 510 chunks\n",
        "          for si in range (num_iter):\n",
        "            offset = si * MAX_LEN\n",
        "            for si_i in range (MAX_LEN):\n",
        "              ct_seq_list.append(last_part_ct_sent[offset+si_i])\n",
        "            ct_token_list.append(2)\n",
        "            for si_j in range(MAX_LEN -2 ):\n",
        "              ct_token_list.append(1)\n",
        "            ct_token_list.append(3)\n",
        "            mas_sequence_list.append(ct_seq_list)\n",
        "            mas_label_list.append(ct_token_list)\n",
        "            ct_seq_list = []\n",
        "            ct_token_list = []\n",
        "          # add the last segment\n",
        "          ct_seq_list = last_part_ct_sent[-last_segment: ]\n",
        "          for si_k in range (last_segment -1 ):\n",
        "            ct_token_list.append(1)\n",
        "          ct_token_list.append(3)\n",
        "          mas_sequence_list.append(ct_seq_list)\n",
        "          mas_label_list.append(ct_token_list)\n",
        "          ct_sequence_length = last_segment\n",
        "\n",
        "  mas_sequence_list.append(ct_seq_list)\n",
        "  mas_label_list.append(ct_token_list)\n",
        "  return mas_sequence_list, mas_label_list, original_sent, tokenized_sent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "GzMCfANS9dp8"
      },
      "outputs": [],
      "source": [
        "def create_attention_masks(master_seq_list):\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in master_seq_list :\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  return attention_masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "5ID2-Abp9emf"
      },
      "outputs": [],
      "source": [
        "def preprocess_small():\n",
        "\n",
        "  extracted_sentences = create_sentences()\n",
        "  mas_sequence_list, mas_label_list, original_sent, tokenized_sent = concatenate_sentences(extracted_sentences)\n",
        "  master_label_list = []\n",
        "\n",
        "  for line in mas_label_list:\n",
        "    #add a padding token to beginning and end\n",
        "    tmp_list = []\n",
        "    tmp_list.append(0)\n",
        "    for tok in line:\n",
        "      tmp_list.append(tok)\n",
        "    tmp_list.append(0)\n",
        "    master_label_list.append(tmp_list)\n",
        "\n",
        "  master_seq_list = []\n",
        "  for line_str in mas_sequence_list:\n",
        "    #add the BERT token for [CLP] and [SEP] to the sentences\n",
        "    tmp_list = []\n",
        "    tmp_list.append(101)\n",
        "    for str_tok in line_str:\n",
        "      tmp_list.append(tokenizer.convert_tokens_to_ids(str_tok))\n",
        "    tmp_list.append(102)\n",
        "    master_seq_list.append(tmp_list)\n",
        "\n",
        "  MAX_LEN = 510\n",
        "\n",
        "  for ri in range (len(master_seq_list[-1]), MAX_LEN+2):\n",
        "    master_seq_list[-1].append(0)\n",
        "\n",
        "  for rj in range (len(master_label_list[-1]), MAX_LEN+2):\n",
        "    master_label_list[-1].append(0)\n",
        "\n",
        "  attention_masks = create_attention_masks(master_seq_list)\n",
        "\n",
        "  return master_seq_list, master_label_list, attention_masks, original_sent, tokenized_sent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "QpVHibW39gpn"
      },
      "outputs": [],
      "source": [
        "def create_train_val(seq_list, labels, masks):\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "  # from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Use 85% for training and 15% for validation.\n",
        "  train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(seq_list, labels, \n",
        "                                                              random_state=seed, test_size=0.15)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, validation_masks, _, _ = train_test_split(masks, labels,\n",
        "                                              random_state=seed, test_size=0.15)# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  return train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hV63NFwt9hxg"
      },
      "outputs": [],
      "source": [
        "def create_tensors(train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks):\n",
        "  train_inputs = torch.tensor(train_inputs)\n",
        "  validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "  train_labels = torch.tensor(train_labels)\n",
        "  validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "  return train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "PIw2lXdf9ivB"
      },
      "outputs": [],
      "source": [
        "def run_tensors():\n",
        "  master_seq_list_small, master_label_list_small, attention_masks_small, original_sents, tokenized_sents = preprocess_small()\n",
        "  train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = create_train_val(master_seq_list_small,master_label_list_small,attention_masks_small)\n",
        "  train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = create_tensors(train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks)\n",
        "\n",
        "  return train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "18yRjMJi9jtP"
      },
      "outputs": [],
      "source": [
        "def create_train_loader(train_inputs, train_labels, train_masks, batch_size):\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, batch_size=batch_size,sampler=train_sampler)\n",
        "\n",
        "  return train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "969gt71-9mkv"
      },
      "outputs": [],
      "source": [
        "def create_val_loader(validation_inputs, validation_labels, validation_masks, batch_size):\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, batch_size=batch_size, sampler=validation_sampler)\n",
        "\n",
        "  return validation_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "OnyHqd_l9sKR"
      },
      "outputs": [],
      "source": [
        "# from seqeval.metrics import f1_score\n",
        "master_mismatch_listOfLists = []\n",
        "\n",
        "def flat_accuracy(preds, labels, att_mask, input_tokens):\n",
        "    total_count = 0\n",
        "    match_count = 0\n",
        "    mismatch = False\n",
        "    temp_list = []\n",
        "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    att_flat = att_mask.flatten ()\n",
        "    input_tkns_flat = input_tokens.flatten()\n",
        "\n",
        "    for i, j_flag in enumerate(att_flat):\n",
        "      if j_flag > 0:\n",
        "        total_count = total_count + 1\n",
        "        if pred_flat[i] == labels_flat[i]:\n",
        "          match_count = match_count + 1\n",
        "        else:\n",
        "          mismatch = True\n",
        "    if mismatch == True:\n",
        "      input_id_list = input_tkns_flat.tolist() \n",
        "      temp_list.append(tokenizer.convert_ids_to_tokens(input_id_list))\n",
        "      temp_list.append(pred_flat.tolist())\n",
        "      temp_list.append (labels_flat.tolist())\n",
        "      master_mismatch_listOfLists.append(temp_list)\n",
        "    return match_count / total_count\n",
        "\n",
        "#flat_accuracy(logits, label_ids, attention_ids, input_token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions to train the model"
      ],
      "metadata": {
        "id": "wjOe8kPeL9Ow"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Y0i1B2k09ylk"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_grad_norm = 1.0\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "\n",
        "def pretrain(model,epochs, batch, learning_rate1, decay, weights=None):\n",
        "  model = model\n",
        "  model.cuda()\n",
        "  model.train()\n",
        "\n",
        "  train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = run_tensors()\n",
        "  train_dataloader = create_train_loader(train_inputs,train_labels,train_masks,batch)\n",
        "  optimizer = AdamW(model.parameters(),betas=(0.9,0.999), lr=learning_rate1,weight_decay=decay, eps=1e-6) \n",
        "\n",
        "  \n",
        "  for name, param in model.named_parameters():\n",
        "    if 'classifier' not in name: # classifier layer\n",
        "      param.requires_grad = False\n",
        "\n",
        "  for _ in trange(epochs, desc=\"Epoch\"):\n",
        "      # TRAIN loop\n",
        "      model.train()\n",
        "      tr_loss = 0\n",
        "      nb_tr_examples, nb_tr_steps = 0, 0\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "          # add batch to gpu\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          # forward pass\n",
        "          outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask, labels=b_labels)\n",
        "          \n",
        "          # model outputs are always tuples in pytorch-transformers\n",
        "          # loss outputs[0], scores\n",
        "          # model outputs are always tuples in pytorch-transformers\n",
        "          # loss, cores = outputs[:2]\n",
        "          loss = outputs[0]\n",
        "          # backward pass\n",
        "          loss.backward()\n",
        "          # track train loss\n",
        "          tr_loss += loss.item()\n",
        "          nb_tr_examples += b_input_ids.size(0)\n",
        "          nb_tr_steps += 1\n",
        "          # gradient clipping\n",
        "          torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "          # update parameters\n",
        "          optimizer.step()\n",
        "          model.zero_grad()\n",
        "      # print train loss per epoch\n",
        "      #print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "  return model\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "8U89L7n_9z9i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#epochs = 5\n",
        "max_grad_norm = 1.0\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "\n",
        "def train(model,epochs,  batch, learning_rate1, learning_rate2,decay):\n",
        "\n",
        "  model = pretrain(model,1, batch, learning_rate1, decay)\n",
        "\n",
        "  # Set bert parameters back to trainable\n",
        "  for name, param in model.named_parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "\n",
        "  train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = run_tensors()\n",
        "  train_dataloader = create_train_loader(train_inputs,train_labels,train_masks,batch)\n",
        "  optimizer = AdamW(model.parameters(),betas=(0.9,0.999), lr=learning_rate2,weight_decay=decay, eps=1e-6) \n",
        "\n",
        "  for _ in trange(epochs, desc=\"Epoch\"):\n",
        "      # TRAIN loop\n",
        "      model.train()\n",
        "      tr_loss = 0\n",
        "      nb_tr_examples, nb_tr_steps = 0, 0\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "          # add batch to gpu\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          # forward pass\n",
        "          outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask, labels=b_labels)\n",
        "          \n",
        "          # model outputs are always tuples in pytorch-transformers\n",
        "          # loss outputs[0], scores\n",
        "          # model outputs are always tuples in pytorch-transformers\n",
        "          # loss, cores = outputs[:2]\n",
        "          loss = outputs[0]\n",
        "          # backward pass\n",
        "          loss.backward()\n",
        "          # track train loss\n",
        "          tr_loss += loss.item()\n",
        "          nb_tr_examples += b_input_ids.size(0)\n",
        "          nb_tr_steps += 1\n",
        "          # gradient clipping\n",
        "          torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "          # update parameters\n",
        "          optimizer.step()\n",
        "          model.zero_grad()\n",
        "      # print train loss per epoch\n",
        "      #print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "  return model\n",
        "\n",
        "#train(model)    \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Vg1nZmtx91I2"
      },
      "outputs": [],
      "source": [
        "def validate(model, batch):\n",
        "  # VALIDATION on validation set\n",
        "  model.eval()\n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  predictions , true_labels = [], []\n",
        "  attention_ids, input_token_ids = [], []\n",
        "  train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = run_tensors()\n",
        "  validation_dataloader = create_train_loader(validation_inputs,validation_labels,validation_masks,batch)\n",
        "  for batch in validation_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      \n",
        "      with torch.no_grad():\n",
        "          output1 = model(b_input_ids, token_type_ids=None,\n",
        "                                attention_mask=b_input_mask, labels=b_labels)\n",
        "          output2 = model(b_input_ids, token_type_ids=None,\n",
        "                          attention_mask=b_input_mask)\n",
        "      tmp_eval_loss = output1[0]\n",
        "      logits = output2[0]\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = b_labels.to('cpu').numpy()\n",
        "      attention_ids = b_input_mask.to('cpu').numpy()\n",
        "      input_token_ids = b_input_ids.to('cpu').numpy()\n",
        "      predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "      true_labels.append(label_ids)\n",
        "      \n",
        "      tmp_eval_accuracy = flat_accuracy(logits, label_ids, attention_ids, input_token_ids)\n",
        "      # identify mismatched texts\n",
        "\n",
        "      eval_loss += tmp_eval_loss.mean().item()\n",
        "      eval_accuracy += tmp_eval_accuracy\n",
        "      \n",
        "      nb_eval_examples += b_input_ids.size(0)\n",
        "      nb_eval_steps += 1\n",
        "\n",
        "  eval_loss = eval_loss/nb_eval_steps\n",
        "  #print(\"Validation loss: {}\".format(eval_loss))\n",
        "  #print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "  return true_labels, predictions\n",
        "\n",
        "#true_labels, predictions = validate(model, validation_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "OnSDlPOT92W0"
      },
      "outputs": [],
      "source": [
        "def train_model(model,epochs, batch, learning_rate1, learning_rate2, decay):\n",
        "  model_train = train(model,epochs, batch, learning_rate1, learning_rate2, decay)\n",
        "  \n",
        "  return model_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "W-BXxyqe93ji"
      },
      "outputs": [],
      "source": [
        "def validate_run(model, batch):\n",
        "  true_labels, predictions = validate(model, batch)\n",
        "\n",
        "  true_x = []\n",
        "  for i_bat in true_labels:\n",
        "    for i_sent in i_bat:\n",
        "      true_x.append(i_sent)\n",
        "\n",
        "  yy_true_list = toTags(true_x)\n",
        "  xx_pred_list = toTags(predictions)  \n",
        "\n",
        "  return f1_score(yy_true_list, xx_pred_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "F-lFnbWG94Vp"
      },
      "outputs": [],
      "source": [
        "def toTags (listOfList) :\n",
        "  retList = []\n",
        "  for sent in listOfList:\n",
        "    tmp_ct_list = []\n",
        "    for token1 in sent:\n",
        "      if (token1 == 0):\n",
        "        tmp_ct_list.append('O')\n",
        "      if (token1 == 2):\n",
        "        tmp_ct_list.append('B-Beg')\n",
        "      if (token1 == 1):\n",
        "        tmp_ct_list.append('I-Inside')\n",
        "      if (token1 == 3):\n",
        "        tmp_ct_list.append('E-End')\n",
        "      if (token1 == 4):\n",
        "        tmp_ct_list.append('B-PAD')\n",
        "    retList.append(tmp_ct_list)\n",
        "  return retList"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions to fine-tune the model. \n",
        "Wandb is used to log the results. May need to create an account on wandb.ai \n",
        "If you want to use different parameters change the below dictionary with the desired values\n"
      ],
      "metadata": {
        "id": "e39OwzokMqvb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "NXVJZSyX96Hh"
      },
      "outputs": [],
      "source": [
        "wandb.login()\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes', #grid, random\n",
        "    'metric': {\n",
        "      'name': 'f1',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [1,2,3,4,5]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [0.000001,0.000005,0.00001,0.00005,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values':[4,6,8]\n",
        "        },\n",
        "        'learning_rate2':\n",
        "        {'values':[0.000001,0.000005,0.00001,0.00005,0.0001,0.0005,0.001,0.005,0.01,0.05]},\n",
        "        'weight_decay': {'values':[0.1,0.05,0.01]}\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YKwFvZ-98S8",
        "outputId": "a74927ef-0cbe-41c3-fb86-905e44f2194b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 5ez25lqc\n",
            "Sweep URL: https://wandb.ai/mgeelen96/SBD%20pretraining/sweeps/5ez25lqc\n"
          ]
        }
      ],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"SBD pretraining\", entity=\"mgeelen96\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 984,
          "referenced_widgets": [
            "5370fdc42c7d40d29c4156947b05e601",
            "bf38e3165acd46b48496ee3247956b21",
            "3cf5a63cb46441668c7f8a7fd45e7235",
            "ae8bd5c66d97431587f43f3059fb906a",
            "c85bd9fc739c4d61842bb4ab503d8b7a",
            "db58d2d68acc427fb7e5d8e7c28cdadb",
            "466a2f026af44b8a8ff071bf05632554",
            "bde7d9d978784aaca87470699bb4ec25",
            "bf124842fcd44ef79362f582c8e8425d",
            "4eefdc19d5c34520ba9865b0d881487d",
            "ff3c48bc88684ecbb260ca773f13480d",
            "d64dc27c16824f15b23d8196c8d1b0b3",
            "2d1818fa82034d9c94561858d49e43a4",
            "a0f91c43ca7d4648808d4bfac1b02fac",
            "6f6871dd470d4459b9b56fa8d2038f3a",
            "95d7fd1c015a4408a62645a6edb5d24e"
          ]
        },
        "id": "g-uypKbH-BhE",
        "outputId": "4a2d1cdc-7049-4a3f-b485-ea1b4092d4c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 4ingejje\n",
            "Sweep URL: https://wandb.ai/mgeelen96/SBD%20pretraining/sweeps/4ingejje\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 82tfmhnk with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/mgeelen96/SBD%20pretraining/runs/82tfmhnk\" target=\"_blank\">dandy-sweep-1</a></strong> to <a href=\"https://wandb.ai/mgeelen96/SBD%20pretraining\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/mgeelen96/SBD%20pretraining/sweeps/4ingejje\" target=\"_blank\">https://wandb.ai/mgeelen96/SBD%20pretraining/sweeps/4ingejje</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 1/1 [00:53<00:00, 53.04s/it]\n",
            "Epoch: 100%|██████████| 2/2 [04:01<00:00, 120.74s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1365... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5370fdc42c7d40d29c4156947b05e601",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1</td><td>0.0</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">dandy-sweep-1</strong>: <a href=\"https://wandb.ai/mgeelen96/SBD%20pretraining/runs/82tfmhnk\" target=\"_blank\">https://wandb.ai/mgeelen96/SBD%20pretraining/runs/82tfmhnk</a><br/>\n",
              "Find logs at: <code>./wandb/run-20211220_192932-82tfmhnk/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f9b0v7cv with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/mgeelen96/SBD%20pretraining/runs/f9b0v7cv\" target=\"_blank\">tough-sweep-2</a></strong> to <a href=\"https://wandb.ai/mgeelen96/SBD%20pretraining\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/mgeelen96/SBD%20pretraining/sweeps/4ingejje\" target=\"_blank\">https://wandb.ai/mgeelen96/SBD%20pretraining/sweeps/4ingejje</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 1/1 [00:58<00:00, 58.05s/it]\n",
            "Epoch: 100%|██████████| 1/1 [02:08<00:00, 128.34s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1422... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf124842fcd44ef79362f582c8e8425d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1</td><td>0.0</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">tough-sweep-2</strong>: <a href=\"https://wandb.ai/mgeelen96/SBD%20pretraining/runs/f9b0v7cv\" target=\"_blank\">https://wandb.ai/mgeelen96/SBD%20pretraining/runs/f9b0v7cv</a><br/>\n",
              "Find logs at: <code>./wandb/run-20211220_193513-f9b0v7cv/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cadicl4s with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/mgeelen96/SBD%20pretraining/runs/cadicl4s\" target=\"_blank\">hardy-sweep-3</a></strong> to <a href=\"https://wandb.ai/mgeelen96/SBD%20pretraining\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/mgeelen96/SBD%20pretraining/sweeps/4ingejje\" target=\"_blank\">https://wandb.ai/mgeelen96/SBD%20pretraining/sweeps/4ingejje</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 1/1 [00:57<00:00, 57.95s/it]\n",
            "Epoch:  20%|██        | 1/5 [02:08<08:33, 128.36s/it]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ],
      "source": [
        "best_f1 = 0\n",
        "\n",
        "def finetune(config=None):\n",
        "  global best_f1\n",
        "  default_config = {\"learning_rate1\":1e-3,\n",
        "                    \"learning_rate2\":1e-3,\n",
        "                  \"epochs\":2,\n",
        "                  \"batch_size\":6,\n",
        "                  \"weight_decay\":0.01}\n",
        "  wandb.init(config=default_config)\n",
        "  config = wandb.config\n",
        "\n",
        "  model_val = train_model(model,config.epochs, config.batch_size, config.learning_rate1, config.learning_rate2,config.weight_decay)      \n",
        "  f1 = validate_run(model_val, config.batch_size)\n",
        "  if f1> best_f1:\n",
        "    torch.save(model_val, \"best_model\")\n",
        "    print(\"model saved\")\n",
        "    best_f1 = f1\n",
        "  wandb.log({\"f1\":f1})\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"SBD pretraining\", entity=\"mgeelen96\")\n",
        "wandb.agent(sweep_id,finetune)\n",
        "        \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Train BERT-model SBD.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5370fdc42c7d40d29c4156947b05e601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bf38e3165acd46b48496ee3247956b21",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3cf5a63cb46441668c7f8a7fd45e7235",
              "IPY_MODEL_ae8bd5c66d97431587f43f3059fb906a"
            ]
          }
        },
        "bf38e3165acd46b48496ee3247956b21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3cf5a63cb46441668c7f8a7fd45e7235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_c85bd9fc739c4d61842bb4ab503d8b7a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db58d2d68acc427fb7e5d8e7c28cdadb"
          }
        },
        "ae8bd5c66d97431587f43f3059fb906a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_466a2f026af44b8a8ff071bf05632554",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bde7d9d978784aaca87470699bb4ec25"
          }
        },
        "c85bd9fc739c4d61842bb4ab503d8b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db58d2d68acc427fb7e5d8e7c28cdadb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "466a2f026af44b8a8ff071bf05632554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bde7d9d978784aaca87470699bb4ec25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf124842fcd44ef79362f582c8e8425d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4eefdc19d5c34520ba9865b0d881487d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ff3c48bc88684ecbb260ca773f13480d",
              "IPY_MODEL_d64dc27c16824f15b23d8196c8d1b0b3"
            ]
          }
        },
        "4eefdc19d5c34520ba9865b0d881487d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff3c48bc88684ecbb260ca773f13480d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_2d1818fa82034d9c94561858d49e43a4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0f91c43ca7d4648808d4bfac1b02fac"
          }
        },
        "d64dc27c16824f15b23d8196c8d1b0b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6f6871dd470d4459b9b56fa8d2038f3a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_95d7fd1c015a4408a62645a6edb5d24e"
          }
        },
        "2d1818fa82034d9c94561858d49e43a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0f91c43ca7d4648808d4bfac1b02fac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f6871dd470d4459b9b56fa8d2038f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "95d7fd1c015a4408a62645a6edb5d24e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}