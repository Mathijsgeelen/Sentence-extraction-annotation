{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mathijsgeelen/Sentence-extraction-annotation/blob/main/Train_BERT_model_SBD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required installations"
      ],
      "metadata": {
        "id": "B6zsHeEfHuRS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayAw4lOX9FgW",
        "outputId": "1a70d806-c3db-418c-eecd-80687f15fb31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 4.1 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 56.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 388 kB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.14.1\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ewKEgyl9JY4",
        "outputId": "d99b71cb-83d6-40ba-9b95-af6f66e0dcb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvnCWyZD9Kh8",
        "outputId": "06f57404-7caf-4c82-9d90-b6c7850d1ad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzZSjaS-9Lq5",
        "outputId": "6c7ab9bd-160c-4ff0-9c75-1d49e3ad0f7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 24.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=4fcb2f05a2f80bd41bdd6f94912ceeedea46fecdf96ae30f66db9c22c716be7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVRYovyOMnJf",
        "outputId": "7c1781ee-f356-41c5-da61-b93375e86d60"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.9-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.1-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 55.2 MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=732eec93e996126ee02e52d492588384c54b55cc1a90fbf5b669bb1780719667\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=4eb6f7677d31728009f78091f85bb832c4cb175539feb186d0df6c466e2277dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.24 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.1 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.9 yaspin-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required imports"
      ],
      "metadata": {
        "id": "kJaRhIgGHybq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "moYME-UJ9Mn4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import AutoModelForTokenClassification, AdamW\n",
        "import random\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
        "from transformers import DebertaTokenizer, DebertaForTokenClassification\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IcuRmYm09PZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487c82ab-3090-44f4-8e15-2b22b1d3b9a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "tf.random.set_seed(seed)\n",
        "TF_DETERMINISTIC_OPS=seed\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found. Please change to a GPU environment')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate desired Transformer model\n",
        "Change the below model and tokenizer if you want to train any other model than DeBERTa"
      ],
      "metadata": {
        "id": "8M4hkrzyLq25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "model = DebertaForTokenClassification.from_pretrained(\"microsoft/deberta-base\", num_labels=4,output_attentions=True)"
      ],
      "metadata": {
        "id": "TRLuKwsiLoeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount google drive containing the 4 file of Savelka et al. (2017).\n",
        "Savelka, Jaromir, Vern R. Walker, Matthias Grabmair and Kevin D. Ashley. \"Sentence Boundary Detection in Adjudicatory Decisions in the Uniter States.\" TAL 58.2 (2017)."
      ],
      "metadata": {
        "id": "FgR7ywg-H3rT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB4Jnj_c9QZT",
        "outputId": "79d29652-425c-4102-df4f-1997c0705b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save all the files in your google drive in a folder named SBD, or change the file paths in the below imports"
      ],
      "metadata": {
        "id": "U1kZi--3IYqb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "26AOnWCJ9S6w"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"/content/drive/My Drive/SBD/bva.json.txt\") as file:\n",
        "  data_bva = json.load(file)\n",
        "with open(\"/content/drive/My Drive/SBD/cyber_crime.json.txt\") as file:\n",
        "  data_cyber = json.load(file)\n",
        "with open(\"/content/drive/My Drive/SBD/intellectual_property.json.txt\") as file:\n",
        "  data_ip = json.load(file)\n",
        "with open(\"/content/drive/My Drive/SBD/scotus.json.txt\") as file:\n",
        "  data_scotus = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information on dataset"
      ],
      "metadata": {
        "id": "5jXbEacPIkbl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38xpjs8Fu1ic"
      },
      "outputs": [],
      "source": [
        "def extract_info():\n",
        "  ### Extracts information on the lenght of the sentences in the dataset\n",
        "  names = [\"BVA\",\"Cyber\",\"IP\",\"Scotus\"]\n",
        "  av = []\n",
        "  for name, data in enumerate([data_bva, data_cyber, data_ip, data_scotus]):\n",
        "    low = 10000\n",
        "    high = 0\n",
        "    average = []\n",
        "    for counter, doc in enumerate(data):\n",
        "      document = data[doc]\n",
        "      for i in range(len(document[\"annotations\"])):\n",
        "        start = document[\"annotations\"][i][\"start\"]\n",
        "        end = document[\"annotations\"][i][\"end\"]\n",
        "        sent = document[\"text\"][start:end+1]\n",
        "        len_sent = len(sent.split(\" \"))\n",
        "        if len_sent < low:\n",
        "          low = len_sent\n",
        "        if len_sent > high:\n",
        "          high = len_sent\n",
        "        average.append(len_sent)\n",
        "        av.append(len_sent)\n",
        "    print(\"Dataset: {}; Smallest sentence: {}, Longest sentence: {}, Average sentence: {:.1f}\".format(names[name],low,high, np.mean(average)))\n",
        "    print()\n",
        "  print(\"Average sentence lenght of entire dataset: {:.1f}\".format(np.mean(av)))\n",
        "\n",
        "extract_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjCrh1xCt_lE"
      },
      "outputs": [],
      "source": [
        "def ends_with_punctuation():\n",
        "  ### extracts information on the number of sentences and characters in the dataset\n",
        "  total_count = 0\n",
        "  total_sentences = 0\n",
        "  names = [\"BVA\",\"Cyber\",\"IP\",\"Scotus\"]\n",
        "  for name, data in enumerate([data_bva, data_cyber, data_ip, data_scotus]):\n",
        "    count = 0\n",
        "    sentences = 0\n",
        "    for counter, doc in enumerate(data):\n",
        "      document = data[doc]\n",
        "      sentences += len(document[\"annotations\"])\n",
        "      total_sentences += len(document[\"annotations\"])\n",
        "      for i in range(len(document[\"annotations\"])):\n",
        "        end = document[\"annotations\"][i][\"end\"]\n",
        "        token = document[\"text\"][end-1]\n",
        "        if token == \".\" or token == \"?\" or token == \"!\":\n",
        "          count+=1\n",
        "          total_count += 1\n",
        "        \n",
        "    print(\"Percentage of sentences that end with punctuation mark in {}: {:.3f}\".format(names[name],count/sentences))\n",
        "  print(\"Percentage of sentences that end with punctuation mark in total dataset: {:.3f}\".format(total_count/total_sentences))\n",
        "\n",
        "\n",
        "ends_with_punctuation()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions to create the dataset"
      ],
      "metadata": {
        "id": "vbWgDU42Is41"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Py0p_CuB9Zq0"
      },
      "outputs": [],
      "source": [
        "def create_sentences():\n",
        "  #create dataset consisting of the individual documents with accompanying sentences split using the annotations.\n",
        "  sentences = []\n",
        "  tags = []\n",
        "  for data in [data_bva, data_cyber, data_ip, data_scotus]:\n",
        "    for counter, doc in enumerate(data):\n",
        "      document = data[doc]\n",
        "      for i in range(len(document[\"annotations\"])):\n",
        "        start = document[\"annotations\"][i][\"start\"]\n",
        "        end = document[\"annotations\"][i][\"end\"]\n",
        "\n",
        "        prev_start = document[\"annotations\"][i-1][\"start\"]\n",
        "        prev_end = document[\"annotations\"][i-1][\"end\"]\n",
        "        sent = document[\"text\"][start:end+1]\n",
        "        prev_sent = document[\"text\"][prev_start:prev_end+1]\n",
        "        if prev_sent[-1] == \" \":\n",
        "          sent = \" \" + sent\n",
        "        if sent[-1] == \" \":\n",
        "          sent = sent[:-1]\n",
        "        sentences.append(sent)\n",
        "\n",
        "  return sentences\n",
        "\n",
        "print(\"Complete dataset contains {} sentences\".format(len(create_sentences())))\n",
        "\n",
        "print(\"This function should return 25899\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I0uJl5OWLhc5"
      },
      "outputs": [],
      "source": [
        "def concatenate_sentences(extracted_sentences):\n",
        "  ### this function loops over the individual sentences and concatenates them to create batches on lenght 512 tokens\n",
        "  MAX_LEN = 510\n",
        "  mas_sequence_list = []\n",
        "  mas_label_list = []\n",
        "  ct_sequence_length = 0\n",
        "  ct_seq_list = []\n",
        "  ct_token_list = []\n",
        "  original_sent = []\n",
        "  tokenized_sent = []\n",
        "  for i, ind_sent in enumerate(extracted_sentences):\n",
        "    # add each sentence to a new sequence of size 510 tokens + 2 required tokens\n",
        "    sent = tokenizer.tokenize(ind_sent)\n",
        "    original_sent.append(ind_sent)\n",
        "    tokenized_sent.append(sent)\n",
        "    \n",
        "    ct_sent_length = len(sent)\n",
        "    if ct_sent_length <= 1:\n",
        "      continue\n",
        "    if (ct_sequence_length + ct_sent_length) < MAX_LEN:\n",
        "      # add the current sentence to current sequence\n",
        "      for item in sent:\n",
        "        ct_seq_list.append (item)\n",
        "      # label 2 indicates begining of a sentence\n",
        "      ct_token_list.append(2)\n",
        "      # label 1 is inside a sentence\n",
        "      for j in range(ct_sent_length - 2):\n",
        "        ct_token_list.append(1)\n",
        "      # label 3 is end of a sentence\n",
        "      ct_token_list.append(3)\n",
        "      ct_sequence_length = ct_sequence_length + ct_sent_length \n",
        "\n",
        "      # for debugging - can remove now\n",
        "      if (len(ct_token_list) != len(ct_seq_list)):\n",
        "        print ('mistmatch in length', i)\n",
        "        print (ct_seq_list)\n",
        "        print (ct_token_list)\n",
        "        print ('labels ', len(ct_token_list))\n",
        "        print ('word-tokens ', len(ct_seq_list))\n",
        "  #    ct_sequence_length + ct_sent_length more than max_length. split sentence\n",
        "  # add one part to current sequence, another to next sequence\n",
        "    else:\n",
        "      # the split sentence is more than 510\n",
        "      num_left_in_ct_seq = MAX_LEN - ct_sequence_length\n",
        "      first_part_ct_sent = sent[ :num_left_in_ct_seq]\n",
        "      last_part_ct_sent = sent [ num_left_in_ct_seq : ]\n",
        "\n",
        "      # following very special case - a second split is needed\n",
        "\n",
        "\n",
        "      # add first part to ct_seq_list\n",
        "      for item2 in first_part_ct_sent:\n",
        "        ct_seq_list.append (item2)\n",
        "      ct_token_list.append(2)\n",
        "      for k in range(num_left_in_ct_seq - 1):\n",
        "        ct_token_list.append(1)\n",
        "\n",
        "\n",
        "        ## deal with exceptions last part is null\n",
        "      if len(last_part_ct_sent) == 0:\n",
        "        ct_token_list[-1] = 3\n",
        "        #print ('zero len', len(ct_seq_list))\n",
        "        mas_sequence_list.append(ct_seq_list)\n",
        "        mas_label_list.append(ct_token_list)\n",
        "        ct_seq_list = []\n",
        "        ct_token_list = []\n",
        "        ct_sequence_length = 0\n",
        "      elif len(last_part_ct_sent) == 1:\n",
        "        #print ('one len', len(ct_seq_list))\n",
        "        mas_sequence_list.append(ct_seq_list)\n",
        "        mas_label_list.append(ct_token_list)\n",
        "        ct_seq_list = []\n",
        "        ct_token_list = []\n",
        "        for item4 in last_part_ct_sent:\n",
        "          ct_seq_list.append (item4)\n",
        "        ct_token_list.append(3)\n",
        "        ct_sequence_length = 1\n",
        "      else:\n",
        "        #print ('normal len', len(ct_seq_list))\n",
        "        mas_sequence_list.append(ct_seq_list)\n",
        "        mas_label_list.append(ct_token_list)\n",
        "        ct_seq_list = []\n",
        "        ct_token_list = []\n",
        "        if len(last_part_ct_sent) < MAX_LEN:\n",
        "          for item3 in last_part_ct_sent:\n",
        "            ct_seq_list.append (item3)\n",
        "            ct_token_list.append(1)\n",
        "          ct_token_list[-1] = 3\n",
        "          ct_sequence_length = len(last_part_ct_sent)\n",
        "        else:     # need to split again.\n",
        "          ## there are 3 sentences that require multiple splits\n",
        "          ## iterate   len (last_part_ct_sent) / MAX_LEN\n",
        "          num_iter = len(last_part_ct_sent) // MAX_LEN\n",
        "          last_segment = len(last_part_ct_sent) % MAX_LEN\n",
        "          #print('split sentence iter  last_seg ', num_iter, last_segment)\n",
        "          # iterate num_iter times to split large into 510 chunks\n",
        "          for si in range (num_iter):\n",
        "            offset = si * MAX_LEN\n",
        "            for si_i in range (MAX_LEN):\n",
        "              ct_seq_list.append(last_part_ct_sent[offset+si_i])\n",
        "            ct_token_list.append(2)\n",
        "            for si_j in range(MAX_LEN -2 ):\n",
        "              ct_token_list.append(1)\n",
        "            ct_token_list.append(3)\n",
        "            mas_sequence_list.append(ct_seq_list)\n",
        "            mas_label_list.append(ct_token_list)\n",
        "            ct_seq_list = []\n",
        "            ct_token_list = []\n",
        "          # add the last segment\n",
        "          ct_seq_list = last_part_ct_sent[-last_segment: ]\n",
        "          for si_k in range (last_segment -1 ):\n",
        "            ct_token_list.append(1)\n",
        "          ct_token_list.append(3)\n",
        "          mas_sequence_list.append(ct_seq_list)\n",
        "          mas_label_list.append(ct_token_list)\n",
        "          ct_sequence_length = last_segment\n",
        "\n",
        "  mas_sequence_list.append(ct_seq_list)\n",
        "  mas_label_list.append(ct_token_list)\n",
        "  return mas_sequence_list, mas_label_list, original_sent, tokenized_sent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GzMCfANS9dp8"
      },
      "outputs": [],
      "source": [
        "def create_attention_masks(master_seq_list):\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in master_seq_list :\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  return attention_masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5ID2-Abp9emf"
      },
      "outputs": [],
      "source": [
        "def preprocess_small():\n",
        "\n",
        "  extracted_sentences = create_sentences()\n",
        "  mas_sequence_list, mas_label_list, original_sent, tokenized_sent = concatenate_sentences(extracted_sentences)\n",
        "  master_label_list = []\n",
        "\n",
        "  for line in mas_label_list:\n",
        "    #add a padding token to beginning and end\n",
        "    tmp_list = []\n",
        "    tmp_list.append(0)\n",
        "    for tok in line:\n",
        "      tmp_list.append(tok)\n",
        "    tmp_list.append(0)\n",
        "    master_label_list.append(tmp_list)\n",
        "\n",
        "  master_seq_list = []\n",
        "  for line_str in mas_sequence_list:\n",
        "    #add the BERT token for [CLP] and [SEP] to the sentences\n",
        "    tmp_list = []\n",
        "    tmp_list.append(101)\n",
        "    for str_tok in line_str:\n",
        "      tmp_list.append(tokenizer.convert_tokens_to_ids(str_tok))\n",
        "    tmp_list.append(102)\n",
        "    master_seq_list.append(tmp_list)\n",
        "\n",
        "  MAX_LEN = 510\n",
        "\n",
        "  for ri in range (len(master_seq_list[-1]), MAX_LEN+2):\n",
        "    master_seq_list[-1].append(0)\n",
        "\n",
        "  for rj in range (len(master_label_list[-1]), MAX_LEN+2):\n",
        "    master_label_list[-1].append(0)\n",
        "\n",
        "  attention_masks = create_attention_masks(master_seq_list)\n",
        "\n",
        "  return master_seq_list, master_label_list, attention_masks, original_sent, tokenized_sent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QpVHibW39gpn"
      },
      "outputs": [],
      "source": [
        "def create_train_val(seq_list, labels, masks):\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "  # from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Use 85% for training and 15% for validation.\n",
        "  train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(seq_list, labels, \n",
        "                                                              random_state=seed, test_size=0.15)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, validation_masks, _, _ = train_test_split(masks, labels,\n",
        "                                              random_state=seed, test_size=0.15)# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  return train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hV63NFwt9hxg"
      },
      "outputs": [],
      "source": [
        "def create_tensors(train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks):\n",
        "  train_inputs = torch.tensor(train_inputs)\n",
        "  validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "  train_labels = torch.tensor(train_labels)\n",
        "  validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "  return train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PIw2lXdf9ivB"
      },
      "outputs": [],
      "source": [
        "def run_tensors():\n",
        "  master_seq_list_small, master_label_list_small, attention_masks_small, original_sents, tokenized_sents = preprocess_small()\n",
        "  train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = create_train_val(master_seq_list_small,master_label_list_small,attention_masks_small)\n",
        "  train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = create_tensors(train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks)\n",
        "\n",
        "  return train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "18yRjMJi9jtP"
      },
      "outputs": [],
      "source": [
        "def create_train_loader(train_inputs, train_labels, train_masks, batch_size):\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, batch_size=batch_size,sampler=train_sampler)\n",
        "\n",
        "  return train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "969gt71-9mkv"
      },
      "outputs": [],
      "source": [
        "def create_val_loader(validation_inputs, validation_labels, validation_masks, batch_size):\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, batch_size=batch_size, sampler=validation_sampler)\n",
        "\n",
        "  return validation_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OnyHqd_l9sKR"
      },
      "outputs": [],
      "source": [
        "# from seqeval.metrics import f1_score\n",
        "master_mismatch_listOfLists = []\n",
        "\n",
        "def flat_accuracy(preds, labels, att_mask, input_tokens):\n",
        "    total_count = 0\n",
        "    match_count = 0\n",
        "    mismatch = False\n",
        "    temp_list = []\n",
        "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    att_flat = att_mask.flatten ()\n",
        "    input_tkns_flat = input_tokens.flatten()\n",
        "\n",
        "    for i, j_flag in enumerate(att_flat):\n",
        "      if j_flag > 0:\n",
        "        total_count = total_count + 1\n",
        "        if pred_flat[i] == labels_flat[i]:\n",
        "          match_count = match_count + 1\n",
        "        else:\n",
        "          mismatch = True\n",
        "    if mismatch == True:\n",
        "      input_id_list = input_tkns_flat.tolist() \n",
        "      temp_list.append(tokenizer.convert_ids_to_tokens(input_id_list))\n",
        "      temp_list.append(pred_flat.tolist())\n",
        "      temp_list.append (labels_flat.tolist())\n",
        "      master_mismatch_listOfLists.append(temp_list)\n",
        "    return match_count / total_count\n",
        "\n",
        "#flat_accuracy(logits, label_ids, attention_ids, input_token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions to train the model"
      ],
      "metadata": {
        "id": "wjOe8kPeL9Ow"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Y0i1B2k09ylk"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_grad_norm = 1.0\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "\n",
        "def pretrain(model,epochs, batch, learning_rate1, decay, weights=None):\n",
        "  model = model\n",
        "  model.cuda()\n",
        "  model.train()\n",
        "\n",
        "  train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = run_tensors()\n",
        "  train_dataloader = create_train_loader(train_inputs,train_labels,train_masks,batch)\n",
        "  optimizer = AdamW(model.parameters(),betas=(0.9,0.999), lr=learning_rate1,weight_decay=decay, eps=1e-6) \n",
        "\n",
        "  \n",
        "  for name, param in model.named_parameters():\n",
        "    if 'classifier' not in name: # classifier layer\n",
        "      param.requires_grad = False\n",
        "\n",
        "  for _ in trange(epochs, desc=\"Epoch\"):\n",
        "      # TRAIN loop\n",
        "      model.train()\n",
        "      tr_loss = 0\n",
        "      nb_tr_examples, nb_tr_steps = 0, 0\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "          # add batch to gpu\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          # forward pass\n",
        "          outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask, labels=b_labels)\n",
        "          \n",
        "          # model outputs are always tuples in pytorch-transformers\n",
        "          # loss outputs[0], scores\n",
        "          # model outputs are always tuples in pytorch-transformers\n",
        "          # loss, cores = outputs[:2]\n",
        "          loss = outputs[0]\n",
        "          # backward pass\n",
        "          loss.backward()\n",
        "          # track train loss\n",
        "          tr_loss += loss.item()\n",
        "          nb_tr_examples += b_input_ids.size(0)\n",
        "          nb_tr_steps += 1\n",
        "          # gradient clipping\n",
        "          torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "          # update parameters\n",
        "          optimizer.step()\n",
        "          model.zero_grad()\n",
        "      # print train loss per epoch\n",
        "      #print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "  return model\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8U89L7n_9z9i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#epochs = 5\n",
        "max_grad_norm = 1.0\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "\n",
        "def train(model,epochs,  batch, learning_rate1, learning_rate2,decay):\n",
        "\n",
        "  model = pretrain(model,1, batch, learning_rate1, decay)\n",
        "\n",
        "  # Set bert parameters back to trainable\n",
        "  for name, param in model.named_parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "\n",
        "  train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = run_tensors()\n",
        "  train_dataloader = create_train_loader(train_inputs,train_labels,train_masks,batch)\n",
        "  optimizer = AdamW(model.parameters(),betas=(0.9,0.999), lr=learning_rate2,weight_decay=decay, eps=1e-6) \n",
        "\n",
        "  for _ in trange(epochs, desc=\"Epoch\"):\n",
        "      # TRAIN loop\n",
        "      model.train()\n",
        "      tr_loss = 0\n",
        "      nb_tr_examples, nb_tr_steps = 0, 0\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "          # add batch to gpu\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          # forward pass\n",
        "          outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask, labels=b_labels)\n",
        "          \n",
        "          # model outputs are always tuples in pytorch-transformers\n",
        "          # loss outputs[0], scores\n",
        "          # model outputs are always tuples in pytorch-transformers\n",
        "          # loss, cores = outputs[:2]\n",
        "          loss = outputs[0]\n",
        "          # backward pass\n",
        "          loss.backward()\n",
        "          # track train loss\n",
        "          tr_loss += loss.item()\n",
        "          nb_tr_examples += b_input_ids.size(0)\n",
        "          nb_tr_steps += 1\n",
        "          # gradient clipping\n",
        "          torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "          # update parameters\n",
        "          optimizer.step()\n",
        "          model.zero_grad()\n",
        "      # print train loss per epoch\n",
        "      #print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "  return model\n",
        "\n",
        "#train(model)    \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Vg1nZmtx91I2"
      },
      "outputs": [],
      "source": [
        "def validate(model, batch):\n",
        "  # VALIDATION on validation set\n",
        "  model.eval()\n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  predictions , true_labels = [], []\n",
        "  attention_ids, input_token_ids = [], []\n",
        "  train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = run_tensors()\n",
        "  validation_dataloader = create_train_loader(validation_inputs,validation_labels,validation_masks,batch)\n",
        "  for batch in validation_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      \n",
        "      with torch.no_grad():\n",
        "          output1 = model(b_input_ids, token_type_ids=None,\n",
        "                                attention_mask=b_input_mask, labels=b_labels)\n",
        "          output2 = model(b_input_ids, token_type_ids=None,\n",
        "                          attention_mask=b_input_mask)\n",
        "      tmp_eval_loss = output1[0]\n",
        "      logits = output2[0]\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = b_labels.to('cpu').numpy()\n",
        "      attention_ids = b_input_mask.to('cpu').numpy()\n",
        "      input_token_ids = b_input_ids.to('cpu').numpy()\n",
        "      predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "      true_labels.append(label_ids)\n",
        "      \n",
        "      tmp_eval_accuracy = flat_accuracy(logits, label_ids, attention_ids, input_token_ids)\n",
        "      # identify mismatched texts\n",
        "\n",
        "      eval_loss += tmp_eval_loss.mean().item()\n",
        "      eval_accuracy += tmp_eval_accuracy\n",
        "      \n",
        "      nb_eval_examples += b_input_ids.size(0)\n",
        "      nb_eval_steps += 1\n",
        "\n",
        "  eval_loss = eval_loss/nb_eval_steps\n",
        "  #print(\"Validation loss: {}\".format(eval_loss))\n",
        "  #print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "  return true_labels, predictions\n",
        "\n",
        "#true_labels, predictions = validate(model, validation_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OnSDlPOT92W0"
      },
      "outputs": [],
      "source": [
        "def train_model(model,epochs, batch, learning_rate1, learning_rate2, decay):\n",
        "  model_train = train(model,epochs, batch, learning_rate1, learning_rate2, decay)\n",
        "  \n",
        "  return model_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "W-BXxyqe93ji"
      },
      "outputs": [],
      "source": [
        "def validate_run(model, batch):\n",
        "  true_labels, predictions = validate(model, batch)\n",
        "\n",
        "  true_x = []\n",
        "  for i_bat in true_labels:\n",
        "    for i_sent in i_bat:\n",
        "      true_x.append(i_sent)\n",
        "\n",
        "  yy_true_list = toTags(true_x)\n",
        "  xx_pred_list = toTags(predictions)  \n",
        "\n",
        "  return f1_score(yy_true_list, xx_pred_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "F-lFnbWG94Vp"
      },
      "outputs": [],
      "source": [
        "def toTags (listOfList) :\n",
        "  retList = []\n",
        "  for sent in listOfList:\n",
        "    tmp_ct_list = []\n",
        "    for token1 in sent:\n",
        "      if (token1 == 0):\n",
        "        tmp_ct_list.append('O')\n",
        "      if (token1 == 2):\n",
        "        tmp_ct_list.append('B-Beg')\n",
        "      if (token1 == 1):\n",
        "        tmp_ct_list.append('I-Inside')\n",
        "      if (token1 == 3):\n",
        "        tmp_ct_list.append('E-End')\n",
        "      if (token1 == 4):\n",
        "        tmp_ct_list.append('B-PAD')\n",
        "    retList.append(tmp_ct_list)\n",
        "  return retList"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions to fine-tune the model. \n",
        "Wandb is used to log the results. May need to create an account on wandb.ai \n",
        "If you want to use different parameters change the below dictionary with the desired values\n"
      ],
      "metadata": {
        "id": "e39OwzokMqvb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXVJZSyX96Hh"
      },
      "outputs": [],
      "source": [
        "wandb.login()\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes', #grid, random\n",
        "    'metric': {\n",
        "      'name': 'f1',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [1,2,3,4,5]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [0.000001,0.000005,0.00001,0.00005,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values':[4,6,8]\n",
        "        },\n",
        "        'learning_rate2':\n",
        "        {'values':[0.000001,0.000005,0.00001,0.00005,0.0001,0.0005,0.001,0.005,0.01,0.05]},\n",
        "        'weight_decay': {'values':[0.1,0.05,0.01]}\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to save the fine-tuning process in your wandb account, add the parameter entity=\"your account name\" to the sweep_id"
      ],
      "metadata": {
        "id": "IuGW_MTzQTJ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YKwFvZ-98S8"
      },
      "outputs": [],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"SBD pretraining\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-uypKbH-BhE"
      },
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "\n",
        "def finetune(config=None):\n",
        "  global best_f1\n",
        "  default_config = {\"learning_rate1\":1e-3,\n",
        "                    \"learning_rate2\":1e-3,\n",
        "                  \"epochs\":2,\n",
        "                  \"batch_size\":6,\n",
        "                  \"weight_decay\":0.01}\n",
        "  wandb.init(config=default_config)\n",
        "  config = wandb.config\n",
        "\n",
        "  model_val = train_model(model,config.epochs, config.batch_size, config.learning_rate1, config.learning_rate2,config.weight_decay)      \n",
        "  f1 = validate_run(model_val, config.batch_size)\n",
        "  if f1> best_f1:\n",
        "    torch.save(model_val, \"best_model\")\n",
        "    print(\"model saved\")\n",
        "    best_f1 = f1\n",
        "  wandb.log({\"f1\":f1})\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"SBD pretraining\")\n",
        "wandb.agent(sweep_id,finetune)\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QFvz03nnQPnZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "B6zsHeEfHuRS",
        "kJaRhIgGHybq",
        "8M4hkrzyLq25",
        "FgR7ywg-H3rT",
        "5jXbEacPIkbl",
        "vbWgDU42Is41",
        "wjOe8kPeL9Ow",
        "e39OwzokMqvb"
      ],
      "machine_shape": "hm",
      "name": "Train BERT-model SBD.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}