{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extract-annotate.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7XROX7O66niW",
        "NL4Ov-1Q73YF",
        "YI_yR8PUs_bQ"
      ],
      "authorship_tag": "ABX9TyMWyx1YMlZdYKP+lpB6xoYg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mathijsgeelen/Sentence-extraction-annotation/blob/main/Extract_annotate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XROX7O66niW"
      },
      "source": [
        "# Required installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCcy2bYBmA1Q"
      },
      "source": [
        "pip install layoutparser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YTT_SCtHnRh"
      },
      "source": [
        "pip install \"layoutparser[effdet]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JG05wMJHonQ"
      },
      "source": [
        "pip install layoutparser torchvision && pip install \"git+https://github.com/facebookresearch/detectron2.git@v0.5#egg=detectron2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8o7D0TnHp9n"
      },
      "source": [
        "pip install \"layoutparser[paddledetection]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjX4N582HrTu"
      },
      "source": [
        "pip install \"layoutparser[ocr]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARn_NtOhHsS9"
      },
      "source": [
        "pip install vila"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChmP0OytHtmP"
      },
      "source": [
        "!apt-get install poppler-utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irSZlWukI_Yp"
      },
      "source": [
        "pip install torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwe7wtPqLPhc"
      },
      "source": [
        "! apt install tesseract-ocr\n",
        "! apt install libtesseract-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAuUHblYLSd9"
      },
      "source": [
        "! pip install Pillow\n",
        "! pip install pytesseract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op1G_mKipMD7"
      },
      "source": [
        "pip install fitz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iofcyztzpL_S"
      },
      "source": [
        "pip install PyMuPDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "O7KPauF6fEjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install truecase"
      ],
      "metadata": {
        "id": "O-OTUvpsfFuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2klcZHyN6tkI"
      },
      "source": [
        "# Required imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4BGqHniHuu5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJwjhedBJEBq"
      },
      "source": [
        "import torch\n",
        "import fitz\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import AutoModelForTokenClassification, AdamW\n",
        "import random\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer, DebertaTokenizer, DebertaForTokenClassification, RobertaForTokenClassification, AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import pandas as pd\n",
        "import pytesseract\n",
        "import layoutparser as lp \n",
        "import pdf2image\n",
        "ocr_agent = lp.TesseractAgent(languages='eng')\n",
        "from vila.pdftools.pdf_extractor import PDFExtractor\n",
        "from vila.predictors import HierarchicalPDFPredictor\n",
        "import truecase\n",
        "import nltk\n",
        "#download punkt\n",
        "nltk.download('punkt')\n",
        "import spacy\n",
        "from spacy import displacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh206RncJG0U"
      },
      "source": [
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "tf.random.set_seed(seed)\n",
        "TF_DETERMINISTIC_OPS=seed\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File paths\n",
        "Add the file path to the DeBERTa sentence boundary detection model which can be downloaded from https://github.com/Mathijsgeelen/Sentence-extraction-annotation/blob/main/DeBERTa_legal_SBD , or use your own SBD model. Upload the model to your google drive or manually and change the paths below.\n",
        "\n",
        "Change file_name to the desired PDF file to be analysed"
      ],
      "metadata": {
        "id": "bHFG4-E2DK0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_sbd = torch.load(\"/content/drive/My Drive/extract-annotate/DeBERTa_legal_SBD\")\n",
        "file_path = \"/content/drive/My Drive/extract-annotate/pdf/legal.pdf\""
      ],
      "metadata": {
        "id": "JnG9lXf9DJBh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cINiOa7U7GgM"
      },
      "source": [
        "# Word and block-level DLA example visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRPNOEMxFqOn"
      },
      "source": [
        "pdf_extractor = PDFExtractor(\"pdfplumber\")\n",
        "page_tokens, page_images = pdf_extractor.load_tokens_and_image(file_path)\n",
        "\n",
        "vision_model = lp.EfficientDetLayoutModel(\"lp://PubLayNet\")\n",
        "pdf_predictor = HierarchicalPDFPredictor.from_pretrained(\"allenai/hvila-row-layoutlm-finetuned-docbank\")\n",
        "\n",
        "ind = 0\n",
        "blocks = vision_model.detect(page_images[ind])\n",
        "page_tokens[ind].annotate(blocks=blocks)\n",
        "pdf_data = page_tokens[ind].to_pagedata().to_dict()\n",
        "predicted_tokens = pdf_predictor.predict(pdf_data)\n",
        "lp.draw_box(page_images[ind], predicted_tokens, box_width=3, box_alpha=0.25) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PFo26_FImT_"
      },
      "source": [
        "import pdf2image\n",
        "image = pdf2image.convert_from_path(file_path)\n",
        "ind = 0\n",
        "image = image[ind]\n",
        "\n",
        "model_layout = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config',\n",
        "                                 extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.50],\n",
        "                                 label_map={0: \"Text\", 1: \"Title\", 2: \"List\"})\n",
        "\n",
        "layout = model_layout.detect(image)\n",
        "#lp.draw_box(image, layout, box_width=3)\n",
        "text_blocks = lp.Layout([b for b in layout if b.type=='Text' or b.type==\"List\"])\n",
        "\n",
        "h, w = np.array(image).shape[:2]\n",
        "\n",
        "left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)\n",
        "\n",
        "left_blocks = text_blocks.filter_by(left_interval, center=True)\n",
        "left_blocks.sort(key = lambda b:b.coordinates[1])\n",
        "\n",
        "right_blocks = [b for b in text_blocks if b not in left_blocks]\n",
        "right_blocks.sort(key = lambda b:b.coordinates[1])\n",
        "\n",
        "# And finally combine the two list and add the index\n",
        "# according to the order\n",
        "text_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])\n",
        "\n",
        "lp.draw_box(image, text_blocks,\n",
        "            box_width=3,\n",
        "            show_element_id=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl7wmUcCA8C6"
      },
      "source": [
        "for block in text_blocks:\n",
        "    segment_image = (block\n",
        "                       .pad(left=10, right=10, top=10, bottom=10)\n",
        "                       .crop_image(np.array(image)))\n",
        "        # add padding in each image segment can help\n",
        "        # improve robustness\n",
        "\n",
        "    text = ocr_agent.detect(segment_image)\n",
        "    block.set(text=text, inplace=True)\n",
        "\n",
        "for txt in text_blocks.get_texts():\n",
        "    print(txt, end='\\n---\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smmFUhfe7mZo"
      },
      "source": [
        "# Functions for text extraction module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_gKj8NPEx7L"
      },
      "source": [
        "def extract_text_blocks(file_name, page_number, pdf_predictor, model_layout):\n",
        "\n",
        "  ### transforms desired page from pdf file to an image.\n",
        "  ### Function pdf_predictor is a pre-trained LayoutLM model for detecting words from an image\n",
        "  ### Function model_layout is a pre-trained Mask-RCNN for text block detection from an image\n",
        "\n",
        "  image = pdf2image.convert_from_path(file_name)\n",
        "  ind = page_number\n",
        "  image = image[ind]\n",
        "\n",
        "  ### Extracts all the words from a pdf page and stores this in predicted_tokens\n",
        "  ### Predicted tokens holds all words from left-to-right, top-to-bottom of a page\n",
        "  pdf_extractor = PDFExtractor(\"pdfplumber\")\n",
        "  page_tokens, page_images = pdf_extractor.load_tokens_and_image(file_name)\n",
        "  blocks = model_layout.detect(page_images[ind])\n",
        "  page_tokens[ind].annotate(blocks=blocks)\n",
        "  pdf_data = page_tokens[ind].to_pagedata().to_dict()\n",
        "  predicted_tokens = pdf_predictor.predict(pdf_data)\n",
        "\n",
        "\n",
        "  ### Extracts the text blocks from an image and stores this in text_blocks\n",
        "  ### Text blocks is an iterable containing all the identified text blocks\n",
        "  layout = model_layout.detect(image)\n",
        "  #lp.draw_box(image, layout, box_width=3)\n",
        "  text_blocks = lp.Layout([b for b in layout if b.type=='Text' or b.type=='List'])\n",
        "\n",
        "  h, w = np.array(image).shape[:2]\n",
        "\n",
        "  left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)\n",
        "\n",
        "  left_blocks = text_blocks.filter_by(left_interval, center=True)\n",
        "  left_blocks.sort(key = lambda b:b.coordinates[1])\n",
        "\n",
        "  right_blocks = [b for b in text_blocks if b not in left_blocks]\n",
        "  right_blocks.sort(key = lambda b:b.coordinates[1])\n",
        "\n",
        "  # And finally combine the two list and add the index\n",
        "  # according to the order\n",
        "  text_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])\n",
        "\n",
        "  lp.draw_box(image, text_blocks,\n",
        "              box_width=3,\n",
        "              show_element_id=True)\n",
        "  \n",
        "  return text_blocks, predicted_tokens, image\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL4Ov-1Q73YF"
      },
      "source": [
        "# Functions for SBD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvx1tkURItLB"
      },
      "source": [
        "def create_attention_masks(sent):\n",
        "  # Create attention masks\n",
        "  \n",
        "  # Create the attention mask.\n",
        "  #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "  #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "  attention_masks = [int(token_id > 0) for token_id in sent]\n",
        "  \n",
        "  # Store the attention mask for this sentence.\n",
        "\n",
        "  return attention_masks\n",
        "\n",
        "def create_sentences(tokenized_text, start_index):\n",
        "  tokenized_sent = [101]\n",
        "  end_index = start_index + 510\n",
        "\n",
        "  if start_index < len(tokenized_text) - 510:\n",
        "\n",
        "    tmp = tokenizer.convert_tokens_to_ids(tokenized_text[start_index:end_index])\n",
        "    [tokenized_sent.append(token) for token in tmp]\n",
        "\n",
        "  else:\n",
        "    tmp = tokenizer.convert_tokens_to_ids(tokenized_text[start_index:])\n",
        "    [tokenized_sent.append(token) for token in tmp]\n",
        "\n",
        "  tokenized_sent.append(102)\n",
        "\n",
        "  #if lenght is not long enough add padding\n",
        "  for i in range(len(tokenized_sent), 512):\n",
        "    tokenized_sent.append(0)\n",
        "  attention_masks = create_attention_masks(tokenized_sent)\n",
        "\n",
        "  return tokenized_sent, attention_masks\n",
        "\n",
        "\n",
        "def create_tensors(seq_list, masks):\n",
        "  train_inputs = torch.tensor(seq_list)\n",
        "\n",
        "  train_masks = torch.tensor(masks)\n",
        "\n",
        "\n",
        "  return train_inputs, train_masks\n",
        "\n",
        "def run_tensors(start_index,text):\n",
        "  tokenized_sent, attention_masks = create_sentences(text, start_index)\n",
        "  train_inputs , train_masks = create_tensors(tokenized_sent, attention_masks)\n",
        "  train_inputs = np.reshape(train_inputs,(1,512))\n",
        "  train_masks = np.reshape(train_masks,(1,512))\n",
        "\n",
        "  return train_inputs, train_masks\n",
        "\n",
        "def create_loader(train_inputs, train_masks):\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(train_inputs, train_masks)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, batch_size=1,sampler=train_sampler)\n",
        "\n",
        "  return train_dataloader\n",
        "\n",
        "\n",
        "def create_dict(preds, masks, input):\n",
        "  \n",
        "  total_count = 0\n",
        "  match_count = 0\n",
        "  mismatch = False\n",
        "  pred_flat = np.argmax(preds, axis=2).flatten()\n",
        "  masks_flat = masks.flatten ()\n",
        "  input_tkns_flat = input.flatten()\n",
        "  preds_dict[\"preds\"].append(pred_flat)\n",
        "\n",
        "  i = 0\n",
        "  while i != len(pred_flat):\n",
        "    #for i, mask_token in enumerate(masks_flat):\n",
        "      #check if attention mask is an actual interesting token, i.e. not 0\n",
        "    begin_pred = i\n",
        "    end_pred = i\n",
        "\n",
        "\n",
        "    while pred_flat[begin_pred] !=2 and begin_pred >= 1:\n",
        "      begin_pred -=1\n",
        "    while pred_flat[end_pred] != 3 and end_pred < len(pred_flat)-1:\n",
        "      end_pred += 1\n",
        "\n",
        "\n",
        "    inputs_pred = input_tkns_flat.tolist()[begin_pred:end_pred+1]\n",
        "    tokens_pred = tokenizer.convert_ids_to_tokens(inputs_pred)\n",
        "    text = tokenizer.convert_tokens_to_string(tokens_pred)\n",
        "    \n",
        "    preds_dict[\"inputs_pred\"].append(inputs_pred)\n",
        "    preds_dict[\"tokens_pred\"].append(tokens_pred)\n",
        "    preds_dict[\"pred_begin_end\"].append([begin_pred,end_pred])\n",
        "    preds_dict[\"text\"].append(text)\n",
        "\n",
        "    i = end_pred+1\n",
        "\n",
        "      \n",
        "def predict_boundary(model, start_index, text):\n",
        "  device = torch.device(\"cuda\")\n",
        "  # VALIDATION on validation set\n",
        "  model.eval()\n",
        "  attention_ids, input_token_ids = [], []\n",
        "  text , masks = run_tensors(start_index,text)\n",
        "  predictions = []\n",
        "  validation_dataloader = create_loader(text ,masks)\n",
        "  for batch in validation_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      b_input_ids, b_input_mask = batch\n",
        "\n",
        "      with torch.no_grad():\n",
        "          output2 = model(b_input_ids, token_type_ids=None,\n",
        "                          attention_mask=b_input_mask)\n",
        "\n",
        "      logits = output2[0]\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      attention_ids = b_input_mask.to('cpu').numpy()\n",
        "      input_token_ids = b_input_ids.to('cpu').numpy()\n",
        "\n",
        "      pred_flat = np.argmax(logits, axis=2).flatten()\n",
        "\n",
        "      \n",
        "\n",
        "      return pred_flat\n",
        "      \n",
        "\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions for extracting coordinates"
      ],
      "metadata": {
        "id": "v0rNcg2HEOfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_boundaries(tokenizer, file_name, page_number, pdf_predictor, model_layout,model_sbd):\n",
        "  ### Uses pre-trained DeBERTa for SBD and loops over these predictions to extract the sentence boundaries\n",
        "  ### The text blocks are fed block by block into the SBD model, first being transformed in the proper format and then\n",
        "  ### for each token the class is predicted.\n",
        "  ### All predicted sentence boundaries are saved in preds_dict, which is a dictionary containing all the boundaries\n",
        "\n",
        "  ### First 2 words prior to and after the sentence boundary are extracted and stored in preds_dict\n",
        "  ### preds_dict is a dictionary ultimately containing all the extracted sentence boundaries.\n",
        "\n",
        "  preds_dict = {\"boundaries\":{},\"start_end\":[]}\n",
        "  entry = 0\n",
        "\n",
        "  # Extract text_blocks and tokens from image, takes as input the file_path, the desired page number, pre-trained word-level extractor\n",
        "  # and pre-trained block level extractor.\n",
        "  text_blocks, predicted_tokens, image = extract_text_blocks(file_name, page_number, pdf_predictor, model_layout)\n",
        "\n",
        "  # Extract text from blocks using OCR\n",
        "  for block in text_blocks:\n",
        "    segment_image = (block\n",
        "                       .pad(left=10, right=10, top=10, bottom=10)\n",
        "                       .crop_image(np.array(image)))\n",
        "        # add padding in each image segment can help\n",
        "        # improve robustness\n",
        "\n",
        "    text = ocr_agent.detect(segment_image)\n",
        "    block.set(text=text, inplace=True)\n",
        "\n",
        "  # Loop over the extracted text from the text block.\n",
        "  # Predict sentence boundaries using predict_boundary\n",
        "  # Once a sentence boundary is encountered, extract the first 2 words prior to and 2 words following the sentence boundary and store this\n",
        "  # in the preds_dict. If no words prior or following, store what's possible.\n",
        "  for txt in text_blocks.get_texts():\n",
        "    text_tokenized = tokenizer.tokenize(txt)\n",
        "    preds = predict_boundary(model_sbd,0, text_tokenized)\n",
        "    end = 0\n",
        "    start = 0\n",
        "    s = 1\n",
        "    e = 1\n",
        "    while len(tokenizer.convert_tokens_to_string(text_tokenized[:s]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").split(\" \")) < 4 and s < len(text_tokenized)-1:\n",
        "      s+=1\n",
        "    while len(tokenizer.convert_tokens_to_string(text_tokenized[:s]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").split(\" \")) < 5 and s < len(text_tokenized)-1:\n",
        "      s+=1\n",
        "    while len(tokenizer.convert_tokens_to_string(text_tokenized[-e:]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").split(\" \")) < 4 and e < len(text_tokenized)-1:\n",
        "      e+=1\n",
        "    sent = tokenizer.convert_tokens_to_string(text_tokenized[-e:]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").split(\" \")\n",
        "    while len(tokenizer.convert_tokens_to_string(text_tokenized[-e:]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").split(\" \")) < 5+sent.count(\"\") and e < len(text_tokenized)-1:\n",
        "      e+=1\n",
        "\n",
        "    start_sent = tokenizer.convert_tokens_to_string(text_tokenized[:s-1]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").rstrip().lstrip()\n",
        "    end_sent = tokenizer.convert_tokens_to_string(text_tokenized[-e+1:]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").rstrip().lstrip()\n",
        "    preds_dict[\"start_end\"].append([start_sent,end_sent])\n",
        "\n",
        "    for i,j in enumerate(preds):\n",
        "      if j == 3:\n",
        "        end = i\n",
        "        left = len(text_tokenized) - end\n",
        "        tokens = []\n",
        "        t = 1\n",
        "        while len(tokenizer.convert_tokens_to_string(text_tokenized[end-t:end]).replace(\"\\n\", \" \").replace(\"\\x0c\",\"\").rstrip().lstrip().split(\" \")) < 3 and end-t > 1:\n",
        "          t += 1\n",
        "        while len(tokenizer.convert_tokens_to_string(text_tokenized[end-t:end]).replace(\"\\n\", \" \").replace(\"\\x0c\",\"\").rstrip().lstrip().split(\" \")) < 4 and end-t > 1:\n",
        "          t += 1\n",
        "\n",
        "        text = tokenizer.convert_tokens_to_string(text_tokenized[end-t+1:end]).replace(\"\\n\", \" \").replace(\"\\x0c\",\"\").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "        tokens.append(text)\n",
        "\n",
        "        t = 1\n",
        "        while len(tokenizer.convert_tokens_to_string(text_tokenized[end:end+t]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").rstrip().split(\" \")) < 3 and left-t > 1:\n",
        "          t += 1\n",
        "        while len(tokenizer.convert_tokens_to_string(text_tokenized[end:end+t]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").rstrip().split(\" \")) < 4 and left-t > 1:\n",
        "          t += 1\n",
        "        text = tokenizer.convert_tokens_to_string(text_tokenized[end:end+t-1]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "        tokens.append(text)\n",
        "        preds_dict[\"boundaries\"][entry] = tokens\n",
        "\n",
        "        entry += 1\n",
        "\n",
        "      elif j == 2:\n",
        "        if preds[i-1] != 3:\n",
        "          end = i-1\n",
        "          left = len(text_tokenized) - end\n",
        "          tokens = []\n",
        "          t = 1\n",
        "          while len(tokenizer.convert_tokens_to_string(text_tokenized[end-t:end]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").rstrip().split(\" \")) < 3 and end-t > 1:\n",
        "            t += 1\n",
        "          while len(tokenizer.convert_tokens_to_string(text_tokenized[end-t:end]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").rstrip().split(\" \")) < 4 and end-t > 1:\n",
        "            t += 1\n",
        "          \n",
        "          text = tokenizer.convert_tokens_to_string(text_tokenized[end-t+1:end]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "          tokens.append(text)\n",
        "\n",
        "          t = 1\n",
        "          while len(tokenizer.convert_tokens_to_string(text_tokenized[end:end+t]).replace(\"\\n\",\" \").replace(\"\\x0c\",\" \").rstrip().split(\" \")) < 3 and left-t > 1:\n",
        "            t += 1\n",
        "          while len(tokenizer.convert_tokens_to_string(text_tokenized[end:end+t]).replace(\"\\n\",\" \").replace(\"\\x0c\",\" \").rstrip().split(\" \")) < 4 and left-t > 1:\n",
        "            t += 1\n",
        "          text = tokenizer.convert_tokens_to_string(text_tokenized[end:end+t-1]).replace(\"\\n\",\" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "          tokens.append(text)\n",
        "\n",
        "          preds_dict[\"boundaries\"][entry] = tokens\n",
        "          entry += 1\n",
        "    \n",
        "    text_tokenized2 = list(filter(lambda a: a != 'Č', text_tokenized))\n",
        "    text_tokenized2 = list(filter(lambda a: a != 'Ċ', text_tokenized2)) \n",
        "\n",
        "    # Slight addition to the pre-trained SBD model. If a text blocks ends with ';', als predict a sentence boundary\n",
        "    # This is in line with Savelka et al. (2017) their documentation, but not correctly captured by the SBD model\n",
        "    if len(text_tokenized2) >= 2:\n",
        "      if text_tokenized2[-1] == \";\" or text_tokenized2[-2] == \";\":\n",
        "        t = 1\n",
        "        tokens = []\n",
        "        while len(tokenizer.convert_tokens_to_string(text_tokenized2[-1-t:]).split(\" \")) < 3 and abs(-1-t) < len(text_tokenized2)-1:\n",
        "          t += 1\n",
        "        while len(tokenizer.convert_tokens_to_string(text_tokenized2[-1-t:]).split(\" \")) < 4 and abs(-1-t) < len(text_tokenized2)-1:\n",
        "          t += 1\n",
        "        #####\n",
        "        text = tokenizer.convert_tokens_to_string(text_tokenized2[-1-t+1:]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "        tokens.append(text)\n",
        "        tokens.append(\" \")\n",
        "        preds_dict[\"boundaries\"][entry] = tokens\n",
        "        entry+=1\n",
        "\n",
        "    for i in range(len(text_tokenized)-2):\n",
        "      if tokenizer.convert_tokens_to_string(text_tokenized[i]) == \";\":\n",
        "        if tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \"\\x0c\" or tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \"\\n\":\n",
        "          t = 1\n",
        "          tokens = []\n",
        "          while len(tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+1]).split(\" \")) < 3 and t+1 < len(text_tokenized)-1:\n",
        "            t+=1\n",
        "          while len(tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+1]).split(\" \")) < 4 and t+1 < len(text_tokenized)-1:\n",
        "            t+=1\n",
        "          ###\n",
        "          \n",
        "          text = tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+1]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "          tokens.append(text)\n",
        "          tokens.append(\" \")\n",
        "          preds_dict[\"boundaries\"][entry] = tokens\n",
        "          entry+=1\n",
        "        elif tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \" or\" and (tokenizer.convert_tokens_to_string(text_tokenized[i+2]) == \"\\x0c\" or tokenizer.convert_tokens_to_string(text_tokenized[i+2]) == \"\\n\"):\n",
        "          t = 1\n",
        "          tokens = []\n",
        "          while len(tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+2]).split(\" \")) < 3 and t+1 < len(text_tokenized)-1:\n",
        "            t+=1\n",
        "          while len(tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+2]).split(\" \")) < 4 and t+1 < len(text_tokenized)-1:\n",
        "            t+=1\n",
        "          ##\n",
        "          text = tokenizer.convert_tokens_to_string(text_tokenized[i-t+1:i+2]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "          tokens.append(text)\n",
        "          tokens.append(\" \")\n",
        "          preds_dict[\"boundaries\"][entry] = tokens\n",
        "          entry+=1\n",
        "      if tokenizer.convert_tokens_to_string(text_tokenized[i]) == \".\" and tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \"\\n\" and tokenizer.convert_tokens_to_string(text_tokenized[i+2]) == \"\\x0c\":\n",
        "        t = 1\n",
        "        tokens = []\n",
        "        while len(tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+1]).split(\" \")) < 3 and t+1 < len(text_tokenized)-1:\n",
        "          t+=1\n",
        "        while len(tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+1]).split(\" \")) < 4 and t+1 < len(text_tokenized)-1:\n",
        "          t+=1\n",
        "        ##\n",
        "        text = tokenizer.convert_tokens_to_string(text_tokenized[i-t+1:i+1]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "        tokens.append(text)\n",
        "        tokens.append(\" \")\n",
        "        preds_dict[\"boundaries\"][entry] = tokens\n",
        "        entry+=1\n",
        "\n",
        "  \n",
        "  return preds_dict, predicted_tokens\n",
        "\n",
        "\n",
        "\n",
        "def extract_indexes_boundaries(preds_dict, predicted_tokens):\n",
        "\n",
        "  ### Extract the index from the predicted boundaries in preds_dict by looking it up in the predicted_tokens list\n",
        "  ### Works by string matching. If predicted boundary tokens exist in the predicted_tokens, then extract desired index\n",
        "\n",
        "  indexes = []\n",
        "  for text in range(len(preds_dict[\"boundaries\"])):\n",
        "    for i in range(len(predicted_tokens)-3):\n",
        "      if preds_dict[\"boundaries\"][text][1] != \" \" and preds_dict[\"boundaries\"][text][1] !=\"\":\n",
        "        if len(preds_dict[\"boundaries\"][text][0].split()) >= 2:\n",
        "          if predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\") and predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\") and predicted_tokens[i+2].text == preds_dict[\"boundaries\"][text][1].split()[0].replace(\" \",\"\"):\n",
        "            indexes.append(i+1)\n",
        "          elif get_count(preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\"),preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\"), predicted_tokens) == 1:\n",
        "            if predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\") and predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\"):\n",
        "              indexes.append(i+1)\n",
        "      else:\n",
        "        if len(preds_dict[\"boundaries\"][text][0].split()) == 2:\n",
        "          if predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\") and predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\"):\n",
        "            indexes.append(i+1)\n",
        "          elif predicted_tokens[i+2].text == preds_dict[\"boundaries\"][text][0].split()[-2].replace(\" \",\"\") and predicted_tokens[i+3].text == preds_dict[\"boundaries\"][text][0].split()[-1].replace(\" \",\"\"):\n",
        "            indexes.append(i+3)\n",
        "  \n",
        "  return list(set(indexes))\n",
        "\n",
        "\n",
        "\n",
        "def extract_coordinates(predicted_tokens,x):\n",
        "\n",
        "  # Transform the index into a set of x-y coordinates\n",
        "  # x0,y0 is top left corner, x1,y1 bottom right\n",
        "  x0 = predicted_tokens[x].block.x_1\n",
        "  x1 = predicted_tokens[x].block.x_2\n",
        "  y0 = predicted_tokens[x].block.y_1\n",
        "  y1 = predicted_tokens[x].block.y_2\n",
        "\n",
        "  return x0,y0,x1,y1\n",
        "\n",
        "def get_count(word1,word2,predicted_tokens):\n",
        "  count = 0\n",
        "  for i in range(len(predicted_tokens)-1):\n",
        "    if predicted_tokens[i].text == word1:\n",
        "      if predicted_tokens[i+1].text == word2:\n",
        "        count += 1\n",
        "\n",
        "  return count\n",
        "\n",
        "\n",
        "def extract_indexes_boundaries(preds_dict, predicted_tokens):\n",
        "\n",
        "  ### Extract the index from the predicted boundaries in preds_dict by looking it up in the predicted_tokens list\n",
        "  ### Works by string matching. If predicted boundary tokens exist in the predicted_tokens, then extract desired index\n",
        "\n",
        "  indexes = []\n",
        "  for text in range(len(preds_dict[\"boundaries\"])):\n",
        "    for i in range(len(predicted_tokens)-3):\n",
        "      if preds_dict[\"boundaries\"][text][1] != \" \" and preds_dict[\"boundaries\"][text][1] !=\"\":\n",
        "        if len(preds_dict[\"boundaries\"][text][0].split()) == 3:\n",
        "          if predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\") and predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\") and predicted_tokens[i+2].text == preds_dict[\"boundaries\"][text][0].split()[2].replace(\" \",\"\"):\n",
        "            indexes.append(i+2)\n",
        "\n",
        "\n",
        "        if len(preds_dict[\"boundaries\"][text][0].split()) == 2:\n",
        "          if predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\") and predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\"):\n",
        "            indexes.append(i+1)\n",
        "          elif get_count(preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\"),preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\"), predicted_tokens) == 1:\n",
        "            if predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\") and predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\"):\n",
        "              indexes.append(i+1)\n",
        "      else:\n",
        "        if len(preds_dict[\"boundaries\"][text][0].split()) == 2:\n",
        "          if predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\") and predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\"):\n",
        "            indexes.append(i+1)\n",
        "          elif predicted_tokens[i+2].text == preds_dict[\"boundaries\"][text][0].split()[-2].replace(\" \",\"\") and predicted_tokens[i+3].text == preds_dict[\"boundaries\"][text][0].split()[-1].replace(\" \",\"\"):\n",
        "            indexes.append(i+3)\n",
        "  \n",
        "  return list(set(indexes))\n",
        "\n",
        "\n",
        "def get_end_index(predicted_tokens, words):\n",
        "  if len(words) >= 3:\n",
        "    for i in range(len(predicted_tokens)):\n",
        "      if predicted_tokens[i].text == words[-1]:\n",
        "        if predicted_tokens[i-1].text == words[-2]:\n",
        "          if predicted_tokens[i-2].text == words[-3]:\n",
        "            if len(words) > 3:\n",
        "              if predicted_tokens[i-3].text == words[-4]:\n",
        "                return i\n",
        "            else:\n",
        "              return i\n",
        "\n",
        "\n",
        "def get_start_index(predicted_tokens, words):\n",
        "  if len(words) >= 3:\n",
        "    for i in range(len(predicted_tokens)-3):\n",
        "      if predicted_tokens[i].text == words[0]:\n",
        "        if predicted_tokens[i+1].text == words[1]:\n",
        "          if predicted_tokens[i+2].text == words[2]:\n",
        "            if len(words) > 3:\n",
        "              if predicted_tokens[i+3].text == words[3]:\n",
        "                return i\n",
        "            else:\n",
        "              return i\n",
        "\n",
        "def extract_indexes_end(sentence, predicted_tokens):\n",
        "  indexes = []\n",
        "  for i in reversed(range(4,len(predicted_tokens))):\n",
        "    if len(sentence.split(\" \")) == 4:\n",
        "      if predicted_tokens[i].text == sentence.split(\" \")[-1].replace(\" \",\"\") and predicted_tokens[i-1].text == sentence.split(\" \")[-2].replace(\" \",\"\") and predicted_tokens[i-2].text == sentence.split(\" \")[-3].replace(\" \",\"\") and predicted_tokens[i-3].text == sentence.split(\" \")[-4].replace(\" \",\"\"):\n",
        "        indexes.append(i)\n",
        "    elif len(sentence.split(\" \")) == 3:\n",
        "      if predicted_tokens[i].text == sentence.split(\" \")[-1].replace(\" \",\"\") and predicted_tokens[i-1].text == sentence.split(\" \")[-2].replace(\" \",\"\") and predicted_tokens[i-2].text == sentence.split(\" \")[-3].replace(\" \",\"\"):\n",
        "        indexes.append(i)\n",
        "\n",
        "\n",
        "  return list(set(indexes))\n",
        "\n",
        "\n",
        "def extract_indexes_start(sentence,predicted_tokens):\n",
        "  indexes = []\n",
        "  for i in range(len(predicted_tokens)-3):\n",
        "      if predicted_tokens[i].text == sentence.split(\" \")[0].replace(\" \",\"\") and predicted_tokens[i+1].text == sentence.split(\" \")[1].replace(\" \",\"\") and predicted_tokens[i+2].text == sentence.split(\" \")[2].replace(\" \",\"\") and predicted_tokens[i+3].text == sentence.split(\" \")[3].replace(\" \",\"\"):\n",
        "        indexes.append(i)\n",
        "  return list(set(indexes))\n",
        "\n",
        "def extract_start_end_indexes(preds_dict, predicted_tokens):\n",
        "  dic = {}\n",
        "  count = 0\n",
        "  for pair in preds_dict[\"start_end\"]:\n",
        "    if len(pair[0].split(\" \")) == 4:\n",
        "      start = extract_indexes_start(pair[0], predicted_tokens)\n",
        "      end = extract_indexes_end(pair[1], predicted_tokens)\n",
        "      stop = False\n",
        "      i = 0\n",
        "      if start != [] and end != []:\n",
        "        if len(start) > 1:\n",
        "          while stop != True and i != len(start):\n",
        "            if start[i] not in dic.keys():\n",
        "              stop = True\n",
        "            else:\n",
        "              i+=1\n",
        "\n",
        "        if len(end) > 1:\n",
        "          for j in end:\n",
        "            if j < start[i]:\n",
        "              end.remove(j)\n",
        "        dic[start[i]] = end\n",
        "\n",
        "  return dic\n",
        "\n",
        "\n",
        "def get_all_sentences_indexes(preds_dict, predicted_tokens):\n",
        "  start_end = extract_start_end_indexes(preds_dict, predicted_tokens)\n",
        "  start = []\n",
        "  end = []\n",
        "  boundaries = []\n",
        "  if start_end != None:\n",
        "    if len(start_end) >= 1:\n",
        "      for key,value in start_end.items():\n",
        "        if key != [] and value != []:\n",
        "          start.append(key)\n",
        "          end.append(value[0])\n",
        "      inbetween = sorted(extract_indexes_boundaries(preds_dict, predicted_tokens))\n",
        "      for token in start:\n",
        "        if token in inbetween:\n",
        "          inbetween.remove(token)\n",
        "      for token in end:\n",
        "        if token in inbetween:\n",
        "          inbetween.remove(token)\n",
        "          \n",
        "      for token in range(len(start)):\n",
        "        tokens = [start[token],end[token]]\n",
        "        for tok in inbetween:\n",
        "          if tok > start[token] and tok < end[token]:\n",
        "            tokens.append(tok)\n",
        "        tokens = sorted(tokens)\n",
        "        for i in range(len(tokens)-1):\n",
        "          boundaries.append([tokens[i],tokens[i+1]])\n",
        "          tokens[i+1] += 1\n",
        "  boundaries.sort(key=lambda x: x[0])\n",
        "    \n",
        "  return boundaries\n"
      ],
      "metadata": {
        "id": "CaHiQyOvTC00"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for NER"
      ],
      "metadata": {
        "id": "YI_yR8PUs_bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NER(all_indexes,predicted_tokens):\n",
        "  ner = spacy.load(\"en_core_web_sm\")\n",
        "  dic = {}\n",
        "\n",
        "  for index in all_indexes:\n",
        "    sent = \"\"\n",
        "    words = []\n",
        "    for ind in range(index[0],index[1]+1):\n",
        "      words.append(predicted_tokens[ind].text)\n",
        "      sent += predicted_tokens[ind].text\n",
        "      sent += \" \"\n",
        "    sent = sent.rstrip()\n",
        "    ner_results = ner(sent)\n",
        "    if len(ner_results) >= 1:\n",
        "      for word in ner_results.ents:\n",
        "        text = word.text\n",
        "        label = word.label_\n",
        "        for i in range(len(text.split(\" \"))):\n",
        "          if text.split(\" \")[i] in words:\n",
        "            index_dict = words.index(text.split(\" \")[i]) + index[0]\n",
        "            dic[index_dict] = label \n",
        "          elif text.split(\" \")[i] + \".\" in words:\n",
        "            index_dict = words.index(text.split(\" \")[i]+\".\") + index[0]\n",
        "            dic[index_dict] = label \n",
        "          elif text.split(\" \")[i] + \"?\" in words:\n",
        "            index_dict = words.index(text.split(\" \")[i]+\"?\") + index[0]\n",
        "            dic[index_dict] = label \n",
        "          elif text.split(\" \")[i] + \"!\" in words:\n",
        "            index_dict = words.index(text.split(\" \")[i]+\"!\") + index[0]\n",
        "            dic[index_dict] = label \n",
        "\n",
        "            \n",
        "\n",
        "  return dic\n",
        "\n"
      ],
      "metadata": {
        "id": "lKjjhExctK4h"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add highlights"
      ],
      "metadata": {
        "id": "isd_CJX1c2x6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_higlights(file_path, model_sbd):\n",
        "  ### Takes as input a file path and adds highlights in that file based on the predicted sentence boundaries.\n",
        "  ### Loops over the extracted coordinates and based on these coordinates highlights text in the file\n",
        "\n",
        "\n",
        "  ### Model for block-level DLA is pre-trained Mask-RCNN on the PubLayNet dataset\n",
        "  model_layout = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config',\n",
        "                                  extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.5],\n",
        "                                  label_map={0: \"Text\", 1: \"Title\", 2: \"List\"})\n",
        "  \n",
        "  ### Model for word-level DLA is pre-trained LayoutLM on the DocBank dataset\n",
        "  pdf_predictor = HierarchicalPDFPredictor.from_pretrained(\"allenai/hvila-block-layoutlm-finetuned-docbank\")\n",
        "  \n",
        "  ### Loops over the file page for page and extracts sentence boundary coordinates after which it highlights these.\n",
        "  pdf_file = fitz.open(file_path)\n",
        "  for p in range(len(pdf_file)):\n",
        "    print(p)\n",
        "    page = pdf_file[p]\n",
        "    preds_dict, predicted_tokens = extract_boundaries(tokenizer, file_name, p, pdf_predictor, model_layout,model_sbd)\n",
        "    indices = extract_indexes_boundaries(preds_dict, predicted_tokens)\n",
        "    all_indexes = get_all_sentences_indexes(preds_dict, predicted_tokens)\n",
        "    ner = NER(all_indexes, predicted_tokens)\n",
        "\n",
        "    for i in all_indexes:\n",
        "      x0,y0,x1,y1 = extract_coordinates(predicted_tokens,i[1])\n",
        "      ul = (x0,y0)\n",
        "      ur = (x1, y0)\n",
        "      ll = (x0,y1)\n",
        "      lr = (x1,y1)\n",
        "      q = fitz.Quad(ul, ur, ll, lr)\n",
        "      page.add_highlight_annot(q)\n",
        "      \n",
        "    for i in indices:\n",
        "      x0,y0,x1,y1 = extract_coordinates(predicted_tokens,i)\n",
        "      ul = (x0,y0)\n",
        "      ur = (x1, y0)\n",
        "      ll = (x0,y1)\n",
        "      lr = (x1,y1)\n",
        "      q = fitz.Quad(ul, ur, ll, lr)\n",
        "      page.add_highlight_annot(q)\n",
        "\n",
        "    for key,value in ner.items():\n",
        "      x0,y0,x1,y1 = extract_coordinates(predicted_tokens,key)\n",
        "      ul = (x0,y0)\n",
        "      ur = (x1, y0)\n",
        "      ll = (x0,y1)\n",
        "      lr = (x1,y1)\n",
        "      q = fitz.Quad(ul, ur, ll, lr)\n",
        "      highlight = page.add_highlight_annot(q)\n",
        "      if value == \"GPE\": #place\n",
        "        highlight.set_colors(stroke=[0.2,1,1]) # light blue\n",
        "        highlight.update()\n",
        "        highlight.update()\n",
        "      elif value == \"DATE\":\n",
        "        highlight.set_colors(stroke=[0.2, 0.7, 0.4]) # green\n",
        "        highlight.update()\n",
        "      elif value == \"PER\":\n",
        "        highlight.set_colors(stroke=[0.3, 0, 0.7]) # purple\n",
        "        highlight.update()\n",
        "      elif value == \"ORG\":\n",
        "        highlight.set_colors(stroke=[0.7, 0.7, 0.8]) # light grey\n",
        "        highlight.update()\n",
        "      elif value == \"ORDINAL\":\n",
        "        highlight.set_colors(stroke=[0.8, 0.5, 0.0]) # light orange\n",
        "        highlight.update()\n",
        "      else:\n",
        "        highlight.set_colors(stroke=[1.0, 0.9, 1.0]) # light pink\n",
        "        highlight.update()\n",
        "\n",
        "\n",
        "  ### File is saved containing the exact same layout as before, but with added highlights\n",
        "  pdf_file.save(\"highlighted.pdf\")\n",
        "    \n",
        "add_higlights(file_path,model_sbd)\n"
      ],
      "metadata": {
        "id": "PDjtsqlWcxLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NCosHmWbUUy8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}