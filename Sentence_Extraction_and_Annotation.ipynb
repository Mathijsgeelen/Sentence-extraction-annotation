{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence Extraction and Annotation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7XROX7O66niW",
        "2klcZHyN6tkI",
        "cINiOa7U7GgM",
        "smmFUhfe7mZo",
        "NL4Ov-1Q73YF",
        "v0rNcg2HEOfp",
        "YI_yR8PUs_bQ"
      ],
      "authorship_tag": "ABX9TyP4WiPIFujhsqBbVpxi6CJU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mathijsgeelen/Sentence-extraction-annotation/blob/main/Sentence_Extraction_and_Annotation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XROX7O66niW"
      },
      "source": [
        "# Required installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCcy2bYBmA1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4310324c-b49b-4327-ec23-d0f90e45edb4"
      },
      "source": [
        "pip install layoutparser"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: layoutparser in /usr/local/lib/python3.7/dist-packages (0.3.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from layoutparser) (9.0.0)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.7/dist-packages (from layoutparser) (0.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from layoutparser) (1.1.5)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.7/dist-packages (from layoutparser) (1.16.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from layoutparser) (6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from layoutparser) (1.4.1)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.7/dist-packages (from layoutparser) (0.1.9)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from layoutparser) (4.1.2.30)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from layoutparser) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from iopath->layoutparser) (4.62.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath->layoutparser) (2.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->layoutparser) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->layoutparser) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->layoutparser) (1.15.0)\n",
            "Requirement already satisfied: Wand>=0.6.7 in /usr/local/lib/python3.7/dist-packages (from pdfplumber->layoutparser) (0.6.7)\n",
            "Requirement already satisfied: pdfminer.six==20211012 in /usr/local/lib/python3.7/dist-packages (from pdfplumber->layoutparser) (20211012)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20211012->pdfplumber->layoutparser) (36.0.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20211012->pdfplumber->layoutparser) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six==20211012->pdfplumber->layoutparser) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six==20211012->pdfplumber->layoutparser) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YTT_SCtHnRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c02cd6-64f6-4853-af18-53b4e203a823"
      },
      "source": [
        "pip install \"layoutparser[effdet]\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: layoutparser[effdet] in /usr/local/lib/python3.7/dist-packages (0.3.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from layoutparser[effdet]) (9.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from layoutparser[effdet]) (1.19.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from layoutparser[effdet]) (4.1.2.30)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from layoutparser[effdet]) (6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from layoutparser[effdet]) (1.4.1)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.7/dist-packages (from layoutparser[effdet]) (1.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from layoutparser[effdet]) (1.1.5)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.7/dist-packages (from layoutparser[effdet]) (0.6.0)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.7/dist-packages (from layoutparser[effdet]) (0.1.9)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from layoutparser[effdet]) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from layoutparser[effdet]) (0.11.1+cu111)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.7/dist-packages (from layoutparser[effdet]) (0.3.0)\n",
            "Requirement already satisfied: timm>=0.4.12 in /usr/local/lib/python3.7/dist-packages (from effdet->layoutparser[effdet]) (0.4.12)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.7/dist-packages (from effdet->layoutparser[effdet]) (2.1.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from effdet->layoutparser[effdet]) (2.0.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from omegaconf>=2.0->effdet->layoutparser[effdet]) (4.8)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[effdet]) (0.29.26)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[effdet]) (57.4.0)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[effdet]) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[effdet]) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[effdet]) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[effdet]) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[effdet]) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[effdet]) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->layoutparser[effdet]) (3.10.0.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath->layoutparser[effdet]) (2.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from iopath->layoutparser[effdet]) (4.62.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->layoutparser[effdet]) (2018.9)\n",
            "Requirement already satisfied: Wand>=0.6.7 in /usr/local/lib/python3.7/dist-packages (from pdfplumber->layoutparser[effdet]) (0.6.7)\n",
            "Requirement already satisfied: pdfminer.six==20211012 in /usr/local/lib/python3.7/dist-packages (from pdfplumber->layoutparser[effdet]) (20211012)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20211012->pdfplumber->layoutparser[effdet]) (3.0.4)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20211012->pdfplumber->layoutparser[effdet]) (36.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six==20211012->pdfplumber->layoutparser[effdet]) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six==20211012->pdfplumber->layoutparser[effdet]) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JG05wMJHonQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379bd0e9-bfe4-4501-fb13-89db30f718b3"
      },
      "source": [
        "pip install layoutparser torchvision && pip install \"git+https://github.com/facebookresearch/detectron2.git@v0.5#egg=detectron2\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: layoutparser in /usr/local/lib/python3.7/dist-packages (0.3.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.7/dist-packages (from layoutparser) (1.16.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from layoutparser) (6.0)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.7/dist-packages (from layoutparser) (0.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from layoutparser) (1.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from layoutparser) (9.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from layoutparser) (1.19.5)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.7/dist-packages (from layoutparser) (0.1.9)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from layoutparser) (4.1.2.30)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from layoutparser) (1.1.5)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from iopath->layoutparser) (4.62.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath->layoutparser) (2.3.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->layoutparser) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->layoutparser) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->layoutparser) (1.15.0)\n",
            "Requirement already satisfied: Wand>=0.6.7 in /usr/local/lib/python3.7/dist-packages (from pdfplumber->layoutparser) (0.6.7)\n",
            "Requirement already satisfied: pdfminer.six==20211012 in /usr/local/lib/python3.7/dist-packages (from pdfplumber->layoutparser) (20211012)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20211012->pdfplumber->layoutparser) (36.0.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20211012->pdfplumber->layoutparser) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six==20211012->pdfplumber->layoutparser) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six==20211012->pdfplumber->layoutparser) (2.21)\n",
            "Collecting detectron2\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git (to revision v0.5) to /tmp/pip-install-wt220rm1/detectron2_4c7f9549ff4346b4918a9b140244b80c\n",
            "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-install-wt220rm1/detectron2_4c7f9549ff4346b4918a9b140244b80c\n",
            "  Running command git checkout -q 82a57ce0b70057685962b352535147d9a8118578\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (9.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2) (3.2.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.0.3)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.0)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.8.9)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2) (4.62.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.7.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.5.post20211023)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: omegaconf>=2.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.1.1)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.1)\n",
            "Requirement already satisfied: black==21.4b2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (21.4b2)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (0.4.3)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (7.1.2)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (3.10.0.2)\n",
            "Requirement already satisfied: pathspec<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (0.9.0)\n",
            "Requirement already satisfied: typed-ast>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (1.5.1)\n",
            "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (2021.11.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (6.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2) (5.4.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2) (4.8)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2) (2.3.2)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2) (0.29.26)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2) (57.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2) (1.15.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core>=1.1->detectron2) (3.7.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.43.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.17.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (4.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8o7D0TnHp9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ae455a-1fdf-4e20-c6bb-2312133f1730"
      },
      "source": [
        "pip install \"layoutparser[paddledetection]\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: layoutparser[paddledetection] in /usr/local/lib/python3.7/dist-packages (0.3.2)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.7/dist-packages (from layoutparser[paddledetection]) (0.1.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from layoutparser[paddledetection]) (1.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from layoutparser[paddledetection]) (4.1.2.30)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from layoutparser[paddledetection]) (1.1.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from layoutparser[paddledetection]) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from layoutparser[paddledetection]) (6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from layoutparser[paddledetection]) (1.19.5)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.7/dist-packages (from layoutparser[paddledetection]) (0.6.0)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.7/dist-packages (from layoutparser[paddledetection]) (1.16.0)\n",
            "Requirement already satisfied: paddlepaddle==2.1.0 in /usr/local/lib/python3.7/dist-packages (from layoutparser[paddledetection]) (2.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==2.1.0->layoutparser[paddledetection]) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==2.1.0->layoutparser[paddledetection]) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==2.1.0->layoutparser[paddledetection]) (3.17.3)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==2.1.0->layoutparser[paddledetection]) (4.4.2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==2.1.0->layoutparser[paddledetection]) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.3.3 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==2.1.0->layoutparser[paddledetection]) (0.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle==2.1.0->layoutparser[paddledetection]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle==2.1.0->layoutparser[paddledetection]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle==2.1.0->layoutparser[paddledetection]) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle==2.1.0->layoutparser[paddledetection]) (1.24.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath->layoutparser[paddledetection]) (2.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from iopath->layoutparser[paddledetection]) (4.62.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->layoutparser[paddledetection]) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->layoutparser[paddledetection]) (2.8.2)\n",
            "Requirement already satisfied: pdfminer.six==20211012 in /usr/local/lib/python3.7/dist-packages (from pdfplumber->layoutparser[paddledetection]) (20211012)\n",
            "Requirement already satisfied: Wand>=0.6.7 in /usr/local/lib/python3.7/dist-packages (from pdfplumber->layoutparser[paddledetection]) (0.6.7)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20211012->pdfplumber->layoutparser[paddledetection]) (36.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six==20211012->pdfplumber->layoutparser[paddledetection]) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six==20211012->pdfplumber->layoutparser[paddledetection]) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjX4N582HrTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "478fd70b-bef7-429d-88ab-a2a46cd71897"
      },
      "source": [
        "pip install \"layoutparser[ocr]\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: layoutparser[ocr] in /usr/local/lib/python3.7/dist-packages (0.3.2)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.7/dist-packages (from layoutparser[ocr]) (0.1.9)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from layoutparser[ocr]) (6.0)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.7/dist-packages (from layoutparser[ocr]) (1.16.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from layoutparser[ocr]) (4.1.2.30)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from layoutparser[ocr]) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from layoutparser[ocr]) (1.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from layoutparser[ocr]) (9.0.0)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.7/dist-packages (from layoutparser[ocr]) (0.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from layoutparser[ocr]) (1.1.5)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.7/dist-packages (from layoutparser[ocr]) (0.3.8)\n",
            "Requirement already satisfied: google-cloud-vision==1 in /usr/local/lib/python3.7/dist-packages (from layoutparser[ocr]) (1.0.0)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-vision==1->layoutparser[ocr]) (1.26.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (1.54.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (57.4.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (21.3)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (1.35.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (1.15.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (1.43.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (4.2.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (3.0.6)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-vision==1->layoutparser[ocr]) (3.0.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from iopath->layoutparser[ocr]) (4.62.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath->layoutparser[ocr]) (2.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->layoutparser[ocr]) (2.8.2)\n",
            "Requirement already satisfied: Wand>=0.6.7 in /usr/local/lib/python3.7/dist-packages (from pdfplumber->layoutparser[ocr]) (0.6.7)\n",
            "Requirement already satisfied: pdfminer.six==20211012 in /usr/local/lib/python3.7/dist-packages (from pdfplumber->layoutparser[ocr]) (20211012)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20211012->pdfplumber->layoutparser[ocr]) (36.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six==20211012->pdfplumber->layoutparser[ocr]) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six==20211012->pdfplumber->layoutparser[ocr]) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARn_NtOhHsS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082cf233-9c19-42fb-f0f1-24c80971800d"
      },
      "source": [
        "pip install vila"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vila in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.7/dist-packages (from vila) (0.3.4)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.7/dist-packages (from vila) (0.6.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (from vila) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from vila) (4.62.3)\n",
            "Requirement already satisfied: torchvision>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from vila) (0.11.1+cu111)\n",
            "Requirement already satisfied: layoutparser>=0.2 in /usr/local/lib/python3.7/dist-packages (from vila) (0.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from vila) (1.1.5)\n",
            "Requirement already satisfied: transformers==4.5 in /usr/local/lib/python3.7/dist-packages (from vila) (4.5.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from vila) (3.6.4)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.7/dist-packages (from vila) (1.16.0)\n",
            "Requirement already satisfied: torch>=1.5 in /usr/local/lib/python3.7/dist-packages (from vila) (1.10.0+cu111)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from vila) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5->vila) (4.10.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5->vila) (0.0.47)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5->vila) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5->vila) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5->vila) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5->vila) (2021.11.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5->vila) (3.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5->vila) (21.3)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.7/dist-packages (from layoutparser>=0.2->vila) (0.1.9)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from layoutparser>=0.2->vila) (4.1.2.30)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from layoutparser>=0.2->vila) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from layoutparser>=0.2->vila) (6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from layoutparser>=0.2->vila) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5->vila) (3.10.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->vila) (0.3.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets->vila) (3.8.1)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->vila) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->vila) (0.70.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets->vila) (0.4.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets->vila) (2022.1.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets->vila) (2.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5->vila) (3.0.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5->vila) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5->vila) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5->vila) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5->vila) (3.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->vila) (5.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->vila) (21.4.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->vila) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->vila) (1.7.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->vila) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->vila) (2.0.10)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->vila) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->vila) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5->vila) (3.7.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath->layoutparser>=0.2->vila) (2.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->vila) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->vila) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->vila) (1.15.0)\n",
            "Requirement already satisfied: Wand>=0.6.7 in /usr/local/lib/python3.7/dist-packages (from pdfplumber->vila) (0.6.7)\n",
            "Requirement already satisfied: pdfminer.six==20211012 in /usr/local/lib/python3.7/dist-packages (from pdfplumber->vila) (20211012)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20211012->pdfplumber->vila) (36.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six==20211012->pdfplumber->vila) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six==20211012->pdfplumber->vila) (2.21)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->vila) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->vila) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->vila) (57.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->vila) (8.12.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->vila) (1.11.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5->vila) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5->vila) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->vila) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChmP0OytHtmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45634203-29b0-4773-b88c-ab20ad167f15"
      },
      "source": [
        "!apt-get install poppler-utils"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (0.62.0-2ubuntu2.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irSZlWukI_Yp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44dc3d8c-b9d4-4cfa-baa3-9aa9175d33ed"
      },
      "source": [
        "pip install torch"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwe7wtPqLPhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8205fb7d-ceac-4dbf-9bcf-b205e89c00cc"
      },
      "source": [
        "! apt install tesseract-ocr\n",
        "! apt install libtesseract-dev"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.00~git2288-10f4998a-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libtesseract-dev is already the newest version (4.00~git2288-10f4998a-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAuUHblYLSd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8615e40-ebd2-4804-96e5-d14b238166c0"
      },
      "source": [
        "! pip install Pillow\n",
        "! pip install pytesseract"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (9.0.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.7/dist-packages (0.3.8)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (9.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op1G_mKipMD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c80fa13-8f77-4ea0-f776-4342e1965262"
      },
      "source": [
        "pip install fitz"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fitz in /usr/local/lib/python3.7/dist-packages (0.0.1.dev2)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.7/dist-packages (from fitz) (5.2.0)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.7/dist-packages (from fitz) (5.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fitz) (1.19.5)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.7/dist-packages (from fitz) (0.17.4)\n",
            "Requirement already satisfied: nipype in /usr/local/lib/python3.7/dist-packages (from fitz) (1.7.0)\n",
            "Requirement already satisfied: pyxnat in /usr/local/lib/python3.7/dist-packages (from fitz) (1.4)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (from fitz) (3.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fitz) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fitz) (1.1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from configobj->fitz) (1.15.0)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (7.1.2)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (1.3.0)\n",
            "Requirement already satisfied: rdflib>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (6.1.1)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (3.17.6)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (2.8.2)\n",
            "Requirement already satisfied: traits!=5.0,>=4.6 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (6.3.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (2.6.3)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (3.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (21.3)\n",
            "Requirement already satisfied: etelemetry>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: prov>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (2.0.0)\n",
            "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.7/dist-packages (from etelemetry>=0.2.0->nipype->fitz) (0.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from etelemetry>=0.2.0->nipype->fitz) (2.23.0)\n",
            "Requirement already satisfied: lxml>=3.3.5 in /usr/local/lib/python3.7/dist-packages (from prov>=1.5.2->nipype->fitz) (4.7.1)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot>=1.2.3->nipype->fitz) (3.0.6)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib>=5.0.0->nipype->fitz) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib>=5.0.0->nipype->fitz) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib>=5.0.0->nipype->fitz) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib>=5.0.0->nipype->fitz) (3.10.0.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fitz) (2018.9)\n",
            "Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.7/dist-packages (from pyxnat->fitz) (0.16.0)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.7/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->etelemetry>=0.2.0->nipype->fitz) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->etelemetry>=0.2.0->nipype->fitz) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->etelemetry>=0.2.0->nipype->fitz) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->etelemetry>=0.2.0->nipype->fitz) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iofcyztzpL_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58dd7f9c-6e35-4832-9042-c147c02c5d98"
      },
      "source": [
        "pip install PyMuPDF"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.7/dist-packages (1.19.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "O7KPauF6fEjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77bcc6f6-8b42-4d55-a857-5a48c20f4cf1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install truecase"
      ],
      "metadata": {
        "id": "O-OTUvpsfFuj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a413aa9-c683-4e80-8e38-9587f74aa7bf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: truecase in /usr/local/lib/python3.7/dist-packages (0.0.14)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from truecase) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->truecase) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2klcZHyN6tkI"
      },
      "source": [
        "# Required imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4BGqHniHuu5",
        "outputId": "1fdb7992-c270-49e3-a5bb-82686d931885"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJwjhedBJEBq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c716a0e-f5ee-4d24-9f28-c26bbcba093d"
      },
      "source": [
        "import torch\n",
        "import fitz\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import AutoModelForTokenClassification, AdamW\n",
        "import random\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer, DebertaTokenizer, DebertaForTokenClassification, RobertaForTokenClassification, AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import pandas as pd\n",
        "import pytesseract\n",
        "import layoutparser as lp \n",
        "import pdf2image\n",
        "ocr_agent = lp.TesseractAgent(languages='eng')\n",
        "from vila.pdftools.pdf_extractor import PDFExtractor\n",
        "from vila.predictors import HierarchicalPDFPredictor\n",
        "import truecase\n",
        "import nltk\n",
        "#download punkt\n",
        "nltk.download('punkt')\n",
        "import spacy\n",
        "from spacy import displacy"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh206RncJG0U",
        "outputId": "f192dc00-8c71-4fc4-b8c8-b79f04daf2ae"
      },
      "source": [
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "tf.random.set_seed(seed)\n",
        "TF_DETERMINISTIC_OPS=seed\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File paths\n",
        "Add the file path to the DeBERTa sentence boundary detection model which can be downloaded from https://github.com/Mathijsgeelen/Sentence-extraction-annotation/blob/main/DeBERTa_legal_SBD , or use your own SBD model. Upload the model to your google drive or manually and change the paths below. When using a different model make sure that change the tokenizer as well.\n",
        "\n",
        "Change file_name to the desired PDF file to be analysed"
      ],
      "metadata": {
        "id": "bHFG4-E2DK0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_sbd = torch.load(\"/content/drive/My Drive/extract-annotate/DeBERTa_legal_SBD\")\n",
        "file_path = \"/content/drive/My Drive/extract-annotate/pdf/legal.pdf\"\n",
        "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")"
      ],
      "metadata": {
        "id": "JnG9lXf9DJBh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cINiOa7U7GgM"
      },
      "source": [
        "# Word and block-level DLA example visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if you get the error \" module 'PIL.TiffTags' has no attribute 'IFD' \" in the below cell, restart the runtime in Google Colab. Or if the run_artefact function does not return anything also restart the runtime. This is an error that often occurs after running the notebook for the first time and is specific to Google Colab"
      ],
      "metadata": {
        "id": "DwF8mcIxVrLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The block below privides token-level boundary box visualizations"
      ],
      "metadata": {
        "id": "eAwQ_WNoWTDu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRPNOEMxFqOn"
      },
      "source": [
        "pdf_extractor = PDFExtractor(\"pdfplumber\")\n",
        "page_tokens, page_images = pdf_extractor.load_tokens_and_image(file_path)\n",
        "\n",
        "vision_model = lp.EfficientDetLayoutModel(\"lp://PubLayNet\")\n",
        "pdf_predictor = HierarchicalPDFPredictor.from_pretrained(\"allenai/hvila-row-layoutlm-finetuned-docbank\")\n",
        "\n",
        "ind = 2\n",
        "blocks = vision_model.detect(page_images[ind])\n",
        "page_tokens[ind].annotate(blocks=blocks)\n",
        "pdf_data = page_tokens[ind].to_pagedata().to_dict()\n",
        "predicted_tokens = pdf_predictor.predict(pdf_data)\n",
        "lp.draw_box(page_images[ind], predicted_tokens, box_width=3, box_alpha=0.25) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below provide block-level visualizations"
      ],
      "metadata": {
        "id": "qCYyvX8IWYcQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PFo26_FImT_"
      },
      "source": [
        "import pdf2image\n",
        "image = pdf2image.convert_from_path(file_path)\n",
        "ind = 5\n",
        "image = image[ind]\n",
        "\n",
        "model_layout = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config',\n",
        "                                 extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.50],\n",
        "                                 label_map={0: \"Text\", 1: \"Title\", 2: \"List\"})\n",
        "\n",
        "layout = model_layout.detect(image)\n",
        "#lp.draw_box(image, layout, box_width=3)\n",
        "text_blocks = lp.Layout([b for b in layout if b.type=='Text' or b.type==\"List\"])\n",
        "\n",
        "h, w = np.array(image).shape[:2]\n",
        "\n",
        "left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)\n",
        "\n",
        "left_blocks = text_blocks.filter_by(left_interval, center=True)\n",
        "left_blocks.sort(key = lambda b:b.coordinates[1])\n",
        "\n",
        "right_blocks = [b for b in text_blocks if b not in left_blocks]\n",
        "right_blocks.sort(key = lambda b:b.coordinates[1])\n",
        "\n",
        "# And finally combine the two list and add the index\n",
        "# according to the order\n",
        "text_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])\n",
        "\n",
        "lp.draw_box(image, text_blocks,\n",
        "            box_width=3,\n",
        "            show_element_id=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below extracts the text from the above created blocks"
      ],
      "metadata": {
        "id": "52VqY0_VWcBz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl7wmUcCA8C6"
      },
      "source": [
        "for block in text_blocks:\n",
        "    segment_image = (block\n",
        "                       .pad(left=12, right=12, top=12, bottom=12)\n",
        "                       .crop_image(np.array(image)))\n",
        "        # add padding in each image segment can help\n",
        "        # improve robustness\n",
        "\n",
        "    text = ocr_agent.detect(segment_image)\n",
        "    block.set(text=text, inplace=True)\n",
        "\n",
        "for txt in text_blocks.get_texts():\n",
        "    print(txt, end='\\n---\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smmFUhfe7mZo"
      },
      "source": [
        "# Functions for text extraction module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_gKj8NPEx7L"
      },
      "source": [
        "def extract_text_blocks(file_name, page_number, pdf_predictor, model_layout):\n",
        "\n",
        "  ### transforms desired page from pdf file to an image.\n",
        "  ### Function pdf_predictor is a pre-trained LayoutLM model for detecting words from an image\n",
        "  ### Function model_layout is a pre-trained Mask-RCNN for text block detection from an image\n",
        "\n",
        "  image = pdf2image.convert_from_path(file_name)\n",
        "  ind = page_number\n",
        "  image = image[ind]\n",
        "\n",
        "  ### Extracts all the words from a pdf page and stores this in predicted_tokens\n",
        "  ### Predicted tokens holds all words from left-to-right, top-to-bottom of a page\n",
        "  pdf_extractor = PDFExtractor(\"pdfplumber\")\n",
        "  page_tokens, page_images = pdf_extractor.load_tokens_and_image(file_name)\n",
        "  blocks = model_layout.detect(page_images[ind])\n",
        "  page_tokens[ind].annotate(blocks=blocks)\n",
        "  pdf_data = page_tokens[ind].to_pagedata().to_dict()\n",
        "  predicted_tokens = pdf_predictor.predict(pdf_data)\n",
        "\n",
        "\n",
        "  ### Extracts the text blocks from an image and stores this in text_blocks\n",
        "  ### Text blocks is an iterable containing all the identified text blocks\n",
        "  layout = model_layout.detect(image)\n",
        "  #lp.draw_box(image, layout, box_width=3)\n",
        "  text_blocks = lp.Layout([b for b in layout if b.type=='Text' or b.type=='List'])\n",
        "\n",
        "  h, w = np.array(image).shape[:2]\n",
        "\n",
        "  left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)\n",
        "\n",
        "  left_blocks = text_blocks.filter_by(left_interval, center=True)\n",
        "  left_blocks.sort(key = lambda b:b.coordinates[1])\n",
        "\n",
        "  right_blocks = [b for b in text_blocks if b not in left_blocks]\n",
        "  right_blocks.sort(key = lambda b:b.coordinates[1])\n",
        "\n",
        "  # And finally combine the two list and add the index\n",
        "  # according to the order\n",
        "  text_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])\n",
        "\n",
        "  lp.draw_box(image, text_blocks,\n",
        "              box_width=3,\n",
        "              show_element_id=True)\n",
        "  \n",
        "  return text_blocks, predicted_tokens, image\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL4Ov-1Q73YF"
      },
      "source": [
        "# Functions for SBD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvx1tkURItLB"
      },
      "source": [
        "def create_attention_masks(sent):\n",
        "  # Create attention masks\n",
        "  \n",
        "  # Create the attention mask.\n",
        "  #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "  #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "  attention_masks = [int(token_id > 0) for token_id in sent]\n",
        "  \n",
        "  # Store the attention mask for this sentence.\n",
        "\n",
        "  return attention_masks\n",
        "\n",
        "def create_sentences(tokenized_text, start_index):\n",
        "  tokenized_sent = [101]\n",
        "  end_index = start_index + 510\n",
        "\n",
        "  if start_index < len(tokenized_text) - 510:\n",
        "\n",
        "    tmp = tokenizer.convert_tokens_to_ids(tokenized_text[start_index:end_index])\n",
        "    [tokenized_sent.append(token) for token in tmp]\n",
        "\n",
        "  else:\n",
        "    tmp = tokenizer.convert_tokens_to_ids(tokenized_text[start_index:])\n",
        "    [tokenized_sent.append(token) for token in tmp]\n",
        "\n",
        "  tokenized_sent.append(102)\n",
        "\n",
        "  #if lenght is not long enough add padding\n",
        "  for i in range(len(tokenized_sent), 512):\n",
        "    tokenized_sent.append(0)\n",
        "  attention_masks = create_attention_masks(tokenized_sent)\n",
        "\n",
        "  return tokenized_sent, attention_masks\n",
        "\n",
        "\n",
        "def create_tensors(seq_list, masks):\n",
        "  train_inputs = torch.tensor(seq_list)\n",
        "\n",
        "  train_masks = torch.tensor(masks)\n",
        "\n",
        "\n",
        "  return train_inputs, train_masks\n",
        "\n",
        "def run_tensors(start_index,text):\n",
        "  tokenized_sent, attention_masks = create_sentences(text, start_index)\n",
        "  train_inputs , train_masks = create_tensors(tokenized_sent, attention_masks)\n",
        "  train_inputs = np.reshape(train_inputs,(1,512))\n",
        "  train_masks = np.reshape(train_masks,(1,512))\n",
        "\n",
        "  return train_inputs, train_masks\n",
        "\n",
        "def create_loader(train_inputs, train_masks):\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(train_inputs, train_masks)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, batch_size=1,sampler=train_sampler)\n",
        "\n",
        "  return train_dataloader\n",
        "\n",
        "\n",
        "def create_dict(preds, masks, input):\n",
        "  \n",
        "  total_count = 0\n",
        "  match_count = 0\n",
        "  mismatch = False\n",
        "  pred_flat = np.argmax(preds, axis=2).flatten()\n",
        "  masks_flat = masks.flatten ()\n",
        "  input_tkns_flat = input.flatten()\n",
        "  preds_dict[\"preds\"].append(pred_flat)\n",
        "\n",
        "  i = 0\n",
        "  while i != len(pred_flat):\n",
        "    #for i, mask_token in enumerate(masks_flat):\n",
        "      #check if attention mask is an actual interesting token, i.e. not 0\n",
        "    begin_pred = i\n",
        "    end_pred = i\n",
        "\n",
        "\n",
        "    while pred_flat[begin_pred] !=2 and begin_pred >= 1:\n",
        "      begin_pred -=1\n",
        "    while pred_flat[end_pred] != 3 and end_pred < len(pred_flat)-1:\n",
        "      end_pred += 1\n",
        "\n",
        "\n",
        "    inputs_pred = input_tkns_flat.tolist()[begin_pred:end_pred+1]\n",
        "    tokens_pred = tokenizer.convert_ids_to_tokens(inputs_pred)\n",
        "    text = tokenizer.convert_tokens_to_string(tokens_pred)\n",
        "    \n",
        "    preds_dict[\"inputs_pred\"].append(inputs_pred)\n",
        "    preds_dict[\"tokens_pred\"].append(tokens_pred)\n",
        "    preds_dict[\"pred_begin_end\"].append([begin_pred,end_pred])\n",
        "    preds_dict[\"text\"].append(text)\n",
        "\n",
        "    i = end_pred+1\n",
        "\n",
        "      \n",
        "def predict_boundary(model, start_index, text):\n",
        "  device = torch.device(\"cuda\")\n",
        "  # VALIDATION on validation set\n",
        "  model.eval()\n",
        "  attention_ids, input_token_ids = [], []\n",
        "  text , masks = run_tensors(start_index,text)\n",
        "  predictions = []\n",
        "  validation_dataloader = create_loader(text ,masks)\n",
        "  for batch in validation_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      b_input_ids, b_input_mask = batch\n",
        "\n",
        "      with torch.no_grad():\n",
        "          output2 = model(b_input_ids, token_type_ids=None,\n",
        "                          attention_mask=b_input_mask)\n",
        "\n",
        "      logits = output2[0]\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      attention_ids = b_input_mask.to('cpu').numpy()\n",
        "      input_token_ids = b_input_ids.to('cpu').numpy()\n",
        "\n",
        "      pred_flat = np.argmax(logits, axis=2).flatten()\n",
        "\n",
        "      \n",
        "\n",
        "      return pred_flat\n",
        "      \n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions for extracting coordinates"
      ],
      "metadata": {
        "id": "v0rNcg2HEOfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_boundaries(tokenizer, file_name, page_number, pdf_predictor, model_layout,model_sbd):\n",
        "  ### Uses pre-trained DeBERTa for SBD and loops over these predictions to extract the sentence boundaries\n",
        "  ### The text blocks are fed block by block into the SBD model, first being transformed in the proper format and then\n",
        "  ### for each token the class is predicted.\n",
        "  ### All predicted sentence boundaries are saved in preds_dict, which is a dictionary containing all the boundaries\n",
        "\n",
        "  ### First 2 words prior to and after the sentence boundary are extracted and stored in preds_dict\n",
        "  ### preds_dict is a dictionary ultimately containing all the extracted sentence boundaries.\n",
        "\n",
        "  preds_dict = {\"boundaries\":{},\"start_end\":[]}\n",
        "  entry = 0\n",
        "\n",
        "  # Extract text_blocks and tokens from image, takes as input the file_path, the desired page number, pre-trained word-level extractor\n",
        "  # and pre-trained block level extractor.\n",
        "  text_blocks, predicted_tokens, image = extract_text_blocks(file_name, page_number, pdf_predictor, model_layout)\n",
        "\n",
        "  # Extract text from blocks using OCR\n",
        "  for block in text_blocks:\n",
        "    segment_image = (block\n",
        "                       .pad(left=12, right=12, top=12, bottom=12)\n",
        "                       .crop_image(np.array(image)))\n",
        "        # add padding in each image segment can help\n",
        "        # improve robustness\n",
        "\n",
        "    text = ocr_agent.detect(segment_image)\n",
        "    block.set(text=text, inplace=True)\n",
        "\n",
        "  # Loop over the extracted text from the text block.\n",
        "  # Predict sentence boundaries using predict_boundary\n",
        "  # Once a sentence boundary is encountered, extract the first 2 words prior to and 2 words following the sentence boundary and store this\n",
        "  # in the preds_dict. If no words prior or following, store what's possible.\n",
        "  for txt in text_blocks.get_texts():\n",
        "    text_tokenized = tokenizer.tokenize(txt)\n",
        "    preds = predict_boundary(model_sbd,0, text_tokenized)\n",
        "    end = 0\n",
        "    start = 0\n",
        "    s = 1\n",
        "    e = 1\n",
        "    if text_tokenized[0] == \".\":\n",
        "      while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[1:s]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").lstrip().split(\" \") if x]) < 4 and s < len(text_tokenized)-1:\n",
        "        s+=1\n",
        "      while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[1:s]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").lstrip().split(\" \") if x]) < 5 and s < len(text_tokenized)-1:\n",
        "        s+=1\n",
        "      start_sent = tokenizer.convert_tokens_to_string(text_tokenized[1:s-1]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").replace('. ',' ').replace(\"““\",\"“\").replace('““','“').replace(\"-\",\"–\").rstrip().lstrip()\n",
        "    elif text_tokenized[0] == \"Â¢\":\n",
        "      while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[1:s]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").lstrip().split(\" \") if x]) < 4 and s < len(text_tokenized)-1:\n",
        "        s+=1\n",
        "      while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[1:s]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").lstrip().split(\" \") if x]) < 5 and s < len(text_tokenized)-1:\n",
        "        s+=1\n",
        "      start_sent = tokenizer.convert_tokens_to_string(text_tokenized[1:s-1]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").replace('  ',' ').replace(\"““\",\"“\").replace('““','“').replace(\"-\",\"–\").rstrip().lstrip()\n",
        "    elif text_tokenized[0] == \"*\":\n",
        "      while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[1:s]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").lstrip().split(\" \") if x]) < 4 and s < len(text_tokenized)-1:\n",
        "        s+=1\n",
        "      while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[1:s]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").lstrip().split(\" \") if x]) < 5 and s < len(text_tokenized)-1:\n",
        "        s+=1\n",
        "      start_sent = tokenizer.convert_tokens_to_string(text_tokenized[1:s-1]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").replace('  ',' ').replace(\"““\",\"“\").replace('““','“').replace(\"-\",\"–\").rstrip().lstrip()\n",
        "    else:\n",
        "      while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[:s]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").lstrip().split(\" \") if x]) < 4 and s < len(text_tokenized)-1:\n",
        "        s+=1\n",
        "      while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[:s]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").lstrip().split(\" \") if x]) < 5 and s < len(text_tokenized)-1:\n",
        "        s+=1\n",
        "      start_sent = tokenizer.convert_tokens_to_string(text_tokenized[:s-1]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").replace('  ',' ').replace(\"““\",\"“\").replace('““','“').replace(\"-\",\"–\").rstrip().lstrip()\n",
        "    while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[-e:]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").split(\" \") if x])  < 4 and e < len(text_tokenized)-1:\n",
        "      e+=1\n",
        "    while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[-e:]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").split(\" \") if x]) < 5 and e < len(text_tokenized)-1:\n",
        "      e+=1\n",
        "    end_sent = tokenizer.convert_tokens_to_string(text_tokenized[-e+1:]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").replace(\"  \",\" \").replace(\"““\",\"“\").replace('““','“').replace(\"-\",\"–\").rstrip().lstrip()\n",
        "    preds_dict[\"start_end\"].append([start_sent,end_sent])\n",
        "\n",
        "    for i,j in enumerate(preds):\n",
        "      if j == 3:\n",
        "        end = i\n",
        "        left = len(text_tokenized) - end\n",
        "        tokens = []\n",
        "        t = 1\n",
        "        while len(tokenizer.convert_tokens_to_string(text_tokenized[end-t:end]).replace(\"\\n\", \" \").replace(\"\\x0c\",\"\").rstrip().lstrip().split(\" \")) < 3 and end-t > 1:\n",
        "          t += 1\n",
        "        while len(tokenizer.convert_tokens_to_string(text_tokenized[end-t:end]).replace(\"\\n\", \" \").replace(\"\\x0c\",\"\").rstrip().lstrip().split(\" \")) < 4 and end-t > 1:\n",
        "          t += 1\n",
        "\n",
        "        text = tokenizer.convert_tokens_to_string(text_tokenized[end-t+1:end]).replace(\"\\n\", \" \").replace(\"\\x0c\",\"\").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "        tokens.append(text)\n",
        "\n",
        "        t = 1\n",
        "        while len(tokenizer.convert_tokens_to_string(text_tokenized[end:end+t]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").rstrip().split(\" \")) < 3 and left-t > 1:\n",
        "          t += 1\n",
        "        while len(tokenizer.convert_tokens_to_string(text_tokenized[end:end+t]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").rstrip().split(\" \")) < 4 and left-t > 1:\n",
        "          t += 1\n",
        "        text = tokenizer.convert_tokens_to_string(text_tokenized[end:end+t-1]).replace(\"\\n\",\" \").replace(\"\\x0c\",\"\").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "        tokens.append(text)\n",
        "        preds_dict[\"boundaries\"][entry] = tokens\n",
        "\n",
        "        entry += 1\n",
        "\n",
        "      elif j == 2:\n",
        "        if preds[i-1] != 3 and (\".\" in text_tokenized[i-2:i-1] or \"!\" in text_tokenized[i-2:i-1] or \"?\" in text_tokenized[i-2:i-1]):\n",
        "          end = i-1\n",
        "          left = len(text_tokenized) - end\n",
        "          tokens = []\n",
        "          t = 1\n",
        "          while len(tokenizer.convert_tokens_to_string(text_tokenized[end-t:end]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").rstrip().split(\" \")) < 3 and end-t > 1:\n",
        "            t += 1\n",
        "          while len(tokenizer.convert_tokens_to_string(text_tokenized[end-t:end]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").rstrip().split(\" \")) < 4 and end-t > 1:\n",
        "            t += 1\n",
        "          \n",
        "          text = tokenizer.convert_tokens_to_string(text_tokenized[end-t+1:end]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "          tokens.append(text)\n",
        "\n",
        "          t = 1\n",
        "          while len(tokenizer.convert_tokens_to_string(text_tokenized[end:end+t]).replace(\"\\n\",\" \").replace(\"\\x0c\",\" \").rstrip().split(\" \")) < 3 and left-t > 1:\n",
        "            t += 1\n",
        "          while len(tokenizer.convert_tokens_to_string(text_tokenized[end:end+t]).replace(\"\\n\",\" \").replace(\"\\x0c\",\" \").rstrip().split(\" \")) < 4 and left-t > 1:\n",
        "            t += 1\n",
        "          text = tokenizer.convert_tokens_to_string(text_tokenized[end:end+t-1]).replace(\"\\n\",\" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "          tokens.append(text)\n",
        "\n",
        "          preds_dict[\"boundaries\"][entry] = tokens\n",
        "          entry += 1\n",
        "    \n",
        "    text_tokenized2 = list(filter(lambda a: a != 'Č', text_tokenized))\n",
        "    text_tokenized2 = list(filter(lambda a: a != 'Ċ', text_tokenized2)) \n",
        "\n",
        "    # Slight addition to the pre-trained SBD model. If a text blocks ends with ';', als predict a sentence boundary\n",
        "    # This is in line with Savelka et al. (2017) their documentation, but not correctly captured by the SBD model\n",
        "\n",
        "    if len(text_tokenized2) >= 2:\n",
        "      if text_tokenized2[-1] == \";\" or text_tokenized2[-2] == \";\":\n",
        "        t = 1\n",
        "        tokens = []\n",
        "        while len(tokenizer.convert_tokens_to_string(text_tokenized2[-1-t:]).split(\" \")) < 3 and abs(-1-t) < len(text_tokenized2)-1:\n",
        "          t += 1\n",
        "        while len(tokenizer.convert_tokens_to_string(text_tokenized2[-1-t:]).split(\" \")) < 4 and abs(-1-t) < len(text_tokenized2)-1:\n",
        "          t += 1\n",
        "        #####\n",
        "        text = tokenizer.convert_tokens_to_string(text_tokenized2[-1-t+1:]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "        tokens.append(text)\n",
        "        tokens.append(\" \")\n",
        "        preds_dict[\"boundaries\"][entry] = tokens\n",
        "        entry+=1\n",
        "\n",
        "    for i in range(len(text_tokenized)-2):\n",
        "      if tokenizer.convert_tokens_to_string(text_tokenized[i]) == \";\":\n",
        "        if tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \"\\x0c\" or tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \"\\n\":\n",
        "          t = 1\n",
        "          tokens = []\n",
        "          while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+1]).split(\" \") if x]) < 3 and t+1 < len(text_tokenized)-1:\n",
        "            t+=1\n",
        "          while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+1]).split(\" \") if x]) < 4 and t+1 < len(text_tokenized)-1:\n",
        "            t+=1\n",
        "          ###\n",
        "          \n",
        "          text = tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+1]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "          tokens.append(text)\n",
        "          tokens.append(\" \")\n",
        "          preds_dict[\"boundaries\"][entry] = tokens\n",
        "          entry+=1\n",
        "        elif (tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \" or\" or tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \" and\" or \\\n",
        "              tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \"and\" or tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \"or\" or \\\n",
        "              tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \"Ġor\" or tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \"Ġand\") \\\n",
        "            and (tokenizer.convert_tokens_to_string(text_tokenized[i+2]) == \"\\x0c\" or tokenizer.convert_tokens_to_string(text_tokenized[i+2]) == \"\\n\"):\n",
        "          t = 1\n",
        "          tokens = []\n",
        "          while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+2]).split(\" \")if x]) < 3 and t+1 < len(text_tokenized)-1:\n",
        "            t+=1\n",
        "          while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+2]).split(\" \") if x]) < 4 and t+1 < len(text_tokenized)-1:\n",
        "            t+=1\n",
        "          ##\n",
        "          text = tokenizer.convert_tokens_to_string(text_tokenized[i-t+1:i+2]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "          tokens.append(text)\n",
        "          tokens.append(\" \")\n",
        "          preds_dict[\"boundaries\"][entry] = tokens\n",
        "          entry+=1\n",
        "        elif (tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \" or\" or tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \" and\" or \\\n",
        "              tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \"and\" or tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \"or\" or \\\n",
        "              tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \"Ġor\" or tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \"Ġand\"):\n",
        "          t = 1\n",
        "          tokens = []\n",
        "          while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+2]).split(\" \")if x]) < 3 and t+1 < len(text_tokenized)-1:\n",
        "            t+=1\n",
        "          while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+2]).split(\" \") if x]) < 4 and t+1 < len(text_tokenized)-1:\n",
        "            t+=1\n",
        "          ##\n",
        "          text = tokenizer.convert_tokens_to_string(text_tokenized[i-t+1:i+2]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "          tokens.append(text)\n",
        "          tokens.append(\" \")\n",
        "          print(text)\n",
        "          preds_dict[\"boundaries\"][entry] = tokens\n",
        "          entry+=1\n",
        "        elif \"(\" in tokenizer.convert_tokens_to_string(text_tokenized[i+1]):\n",
        "          t = 1\n",
        "          tokens = []\n",
        "          while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[i-t:i]).split(\" \")if x]) < 3 and t+1 < len(text_tokenized)-1:\n",
        "            t+=1\n",
        "          while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+1]).split(\" \") if x]) < 4 and t+1 < len(text_tokenized)-1:\n",
        "            t+=1\n",
        "          ##\n",
        "          text = tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+1]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "          tokens.append(text)\n",
        "          tokens.append(\" \")\n",
        "          preds_dict[\"boundaries\"][entry] = tokens\n",
        "          entry+=1\n",
        "\n",
        "        elif tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \" or\" and \"(\" in tokenizer.convert_tokens_to_string(text_tokenized[i+2]):\n",
        "          t = 1\n",
        "          tokens = []\n",
        "          while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[i-t:i]).split(\" \") if x]) < 3 and t+1 < len(text_tokenized)-1:\n",
        "            t+=1\n",
        "          while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+1]).split(\" \") if x]) < 4 and t+1 < len(text_tokenized)-1:\n",
        "            t+=1\n",
        "          ##\n",
        "          text = tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+1]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "          tokens.append(text)\n",
        "          tokens.append(\" \")\n",
        "          preds_dict[\"boundaries\"][entry] = tokens\n",
        "          entry+=1\n",
        "\n",
        "      if tokenizer.convert_tokens_to_string(text_tokenized[i]) == \".\" and tokenizer.convert_tokens_to_string(text_tokenized[i+1]) == \"\\n\" and tokenizer.convert_tokens_to_string(text_tokenized[i+2]) == \"\\x0c\":\n",
        "        t = 1\n",
        "        tokens = []\n",
        "        while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+1]).split(\" \") if x]) < 3 and t+1 < len(text_tokenized)-1:\n",
        "          t+=1\n",
        "        while len([x for x in tokenizer.convert_tokens_to_string(text_tokenized[i-t:i+1]).split(\" \") if x]) < 4 and t+1 < len(text_tokenized)-1:\n",
        "          t+=1\n",
        "        ##\n",
        "        text = tokenizer.convert_tokens_to_string(text_tokenized[i-t+1:i+1]).replace(\"\\n\", \" \").replace(\"\\x0c\",\" \").replace(\"  \",\" \").replace(\"   \",\" \").lstrip().strip()\n",
        "        tokens.append(text)\n",
        "        tokens.append(\" \")\n",
        "        preds_dict[\"boundaries\"][entry] = tokens\n",
        "        entry+=1\n",
        "\n",
        "  \n",
        "  return preds_dict, predicted_tokens\n",
        "\n",
        "\n",
        "\n",
        "def extract_coordinates(predicted_tokens,x):\n",
        "\n",
        "  # Transform the index into a set of x-y coordinates\n",
        "  # x0,y0 is top left corner, x1,y1 bottom right\n",
        "  x0 = predicted_tokens[x].block.x_1\n",
        "  x1 = predicted_tokens[x].block.x_2\n",
        "  y0 = predicted_tokens[x].block.y_1\n",
        "  y1 = predicted_tokens[x].block.y_2\n",
        "\n",
        "  return x0,y0,x1,y1\n",
        "\n",
        "def get_count(word1,word2,predicted_tokens):\n",
        "  count = 0\n",
        "  for i in range(len(predicted_tokens)-1):\n",
        "    if predicted_tokens[i].text == word1:\n",
        "      if predicted_tokens[i+1].text == word2:\n",
        "        count += 1\n",
        "\n",
        "  return count\n",
        "\n",
        "\n",
        "def extract_indexes_boundaries(preds_dict, predicted_tokens):\n",
        "\n",
        "  ### Extract the index from the predicted boundaries in preds_dict by looking it up in the predicted_tokens list\n",
        "  ### Works by string matching. If predicted boundary tokens exist in the predicted_tokens, then extract desired index\n",
        "\n",
        "  indexes = []\n",
        "  for text in range(len(preds_dict[\"boundaries\"])):\n",
        "    for i in range(len(predicted_tokens)-3):\n",
        "      if preds_dict[\"boundaries\"][text][1] != \" \" and preds_dict[\"boundaries\"][text][1] !=\"\":\n",
        "        if len(preds_dict[\"boundaries\"][text][0].split()) == 3:\n",
        "          if (predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\")) and \\\n",
        "              (predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\")) and \\\n",
        "              (predicted_tokens[i+2].text == preds_dict[\"boundaries\"][text][0].split()[2].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[2].replace(\" \",\"\")):\n",
        "            indexes.append(i+2)\n",
        "          elif (predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\")) and \\\n",
        "                (predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\")) and \\\n",
        "                (predicted_tokens[i+2].text == \".\" or predicted_tokens[i+2].text==\"?\" or predicted_tokens[i+2]==\"!\" or \"'\" in preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\")):\n",
        "            indexes.append(i+2)\n",
        "        elif len(preds_dict[\"boundaries\"][text][0].split()) > 3:\n",
        "          if (predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[-3].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[-3].replace(\" \",\"\")) and \\\n",
        "              (predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[-2].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[-2].replace(\" \",\"\")) and \\\n",
        "              (predicted_tokens[i+2].text == preds_dict[\"boundaries\"][text][0].split()[-1].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[-1].replace(\" \",\"\")):\n",
        "            indexes.append(i+2)\n",
        "          elif (predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[-3].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[-3].replace(\" \",\"\")) and \\\n",
        "                (predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[-2].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[-2].replace(\" \",\"\")) and \\\n",
        "                (predicted_tokens[i+2].text == \".\" or predicted_tokens[i+2].text==\"?\" or predicted_tokens[i+2]==\"!\" or \"'\" in preds_dict[\"boundaries\"][text][0].split()[-2].replace(\" \",\"\")):\n",
        "            indexes.append(i+2)\n",
        "\n",
        "\n",
        "        elif len(preds_dict[\"boundaries\"][text][0].split()) == 2:\n",
        "          if (predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\")) and \\\n",
        "              (predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\")):\n",
        "            indexes.append(i+1)\n",
        "          elif get_count(preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\"),preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\"), predicted_tokens) == 1:\n",
        "            if (predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\")) and \\\n",
        "                (predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\")):\n",
        "              indexes.append(i+1)\n",
        "      else:\n",
        "        if len(preds_dict[\"boundaries\"][text][0].split()) == 2:\n",
        "          if predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\") and \\\n",
        "              predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\") and \\\n",
        "              (\".\" in preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\") or \"?\" in preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\") or \"!\" in preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\")):\n",
        "            indexes.append(i+1)\n",
        "          elif predicted_tokens[i+2].text == preds_dict[\"boundaries\"][text][0].split()[-2].replace(\" \",\"\") and predicted_tokens[i+3].text == preds_dict[\"boundaries\"][text][0].split()[-1].replace(\" \",\"\") and (\".\" in preds_dict[\"boundaries\"][text][0].split()[-1].replace(\" \",\"\") or \"?\" in preds_dict[\"boundaries\"][text][0].split()[-1].replace(\" \",\"\") or \"!\" in preds_dict[\"boundaries\"][text][0].split()[-1].replace(\" \",\"\")):\n",
        "            indexes.append(i+3)\n",
        "        elif len(preds_dict[\"boundaries\"][text][0].split()) == 3:\n",
        "          if (predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[0].replace(\" \",\"\")) and \\\n",
        "              (predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[1].replace(\" \",\"\")) and \\\n",
        "              (predicted_tokens[i+2].text == preds_dict[\"boundaries\"][text][0].split()[2].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[2].replace(\" \",\"\")):\n",
        "            indexes.append(i+2)\n",
        "        elif len(preds_dict[\"boundaries\"][text][0].split()) > 3:\n",
        "          if (predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[-3].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[-3].replace(\" \",\"\")) and \\\n",
        "              (predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[-2].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[-2].replace(\" \",\"\")) and \\\n",
        "              (predicted_tokens[i+2].text == preds_dict[\"boundaries\"][text][0].split()[-1].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[-1].replace(\" \",\"\")):\n",
        "            indexes.append(i+2)\n",
        "          elif (predicted_tokens[i].text == preds_dict[\"boundaries\"][text][0].split()[-3].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[-3].replace(\" \",\"\")) and \\\n",
        "                (predicted_tokens[i+1].text == preds_dict[\"boundaries\"][text][0].split()[-2].replace(\" \",\"\") or \"'\" in preds_dict[\"boundaries\"][text][0].split()[-2].replace(\" \",\"\")) and \\\n",
        "                (predicted_tokens[i+2].text == \".\" or predicted_tokens[i+2].text==\"?\" or predicted_tokens[i+2]==\"!\" or \"'\" in preds_dict[\"boundaries\"][text][0].split()[-2].replace(\" \",\"\")):\n",
        "            indexes.append(i+2)\n",
        "  return list(set(indexes))\n",
        "\n",
        "\n",
        "def get_end_index(predicted_tokens, words):\n",
        "  if len(words) >= 3:\n",
        "    for i in range(len(predicted_tokens)):\n",
        "      if predicted_tokens[i].text == words[-1]:\n",
        "        if predicted_tokens[i-1].text == words[-2]:\n",
        "          if predicted_tokens[i-2].text == words[-3]:\n",
        "            if len(words) > 3:\n",
        "              if predicted_tokens[i-3].text == words[-4]:\n",
        "                return i\n",
        "            else:\n",
        "              return i\n",
        "\n",
        "\n",
        "def get_start_index(predicted_tokens, words):\n",
        "  if len(words) >= 3:\n",
        "    for i in range(len(predicted_tokens)-3):\n",
        "      if predicted_tokens[i].text == words[0]:\n",
        "        if predicted_tokens[i+1].text == words[1]:\n",
        "          if predicted_tokens[i+2].text == words[2]:\n",
        "            if len(words) > 3:\n",
        "              if predicted_tokens[i+3].text == words[3]:\n",
        "                return i\n",
        "            else:\n",
        "              return i\n",
        "\n",
        "def extract_indexes_end(sentence, predicted_tokens):\n",
        "  indexes = []\n",
        "  for i in reversed(range(4,len(predicted_tokens))):\n",
        "    #if \".\" in sentence.split(\" \")[-1].replace(\" \",\"\") or \"!\" in sentence.split(\" \")[-1].replace(\" \",\"\") or \"?\" in sentence.split(\" \")[-1].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[-1].replace(\" \",\"\"):\n",
        "    if len(sentence.split(\" \")) == 4:\n",
        "      if (predicted_tokens[i].text == sentence.split(\" \")[-1].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[-1].replace(\" \",\"\")) and \\\n",
        "      (predicted_tokens[i-1].text == sentence.split(\" \")[-2].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[-2].replace(\" \",\"\")) and \\\n",
        "      (predicted_tokens[i-2].text == sentence.split(\" \")[-3].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[-3].replace(\" \",\"\")) and \\\n",
        "      (predicted_tokens[i-3].text == sentence.split(\" \")[-4].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[-4].replace(\" \",\"\")):\n",
        "        indexes.append(i)\n",
        "    elif len(sentence.split(\" \")) == 3:\n",
        "      if (predicted_tokens[i].text == sentence.split(\" \")[-1].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[-1].replace(\" \",\"\")) and \\\n",
        "      (predicted_tokens[i-1].text == sentence.split(\" \")[-2].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[-2].replace(\" \",\"\")) and \\\n",
        "      (predicted_tokens[i-2].text == sentence.split(\" \")[-3].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[-3].replace(\" \",\"\")):\n",
        "        indexes.append(i)\n",
        "\n",
        "\n",
        "  return list(set(indexes))\n",
        "\n",
        "\n",
        "def extract_indexes_start(sentence,predicted_tokens):\n",
        "  indexes = []\n",
        "  for i in range(len(predicted_tokens)-3):\n",
        "    if (predicted_tokens[i].text == sentence.split(\" \")[0].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[0].replace(\" \",\"\")) and \\\n",
        "    (predicted_tokens[i+1].text == sentence.split(\" \")[1].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[1].replace(\" \",\"\")) and \\\n",
        "    (predicted_tokens[i+2].text == sentence.split(\" \")[2].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[2].replace(\" \",\"\")) and \\\n",
        "    (predicted_tokens[i+3].text == sentence.split(\" \")[3].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[3].replace(\" \",\"\")):\n",
        "      indexes.append(i)\n",
        "  return list(set(indexes))\n",
        "\n",
        "def extract_indexes_start2(sentence,predicted_tokens):\n",
        "  indexes = []\n",
        "  for i in range(len(predicted_tokens)-3):\n",
        "    if (predicted_tokens[i].text == sentence.split(\" \")[1].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[1].replace(\" \",\"\")) and \\\n",
        "        (predicted_tokens[i+1].text == sentence.split(\" \")[2].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[2].replace(\" \",\"\")) and \\\n",
        "        (predicted_tokens[i+2].text == sentence.split(\" \")[3].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[3].replace(\" \",\"\")):\n",
        "      indexes.append(i)\n",
        "    elif (predicted_tokens[i].text == sentence.split(\" \")[2].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[2].replace(\" \",\"\")) and \\\n",
        "        (predicted_tokens[i+1].text == sentence.split(\" \")[3].replace(\" \",\"\") or \"'\" in sentence.split(\" \")[3].replace(\" \",\"\")):\n",
        "        indexes.append(i)\n",
        "\n",
        "  return indexes\n",
        "\n",
        "\n",
        "\n",
        "def extract_start_end_indexes(preds_dict, predicted_tokens):\n",
        "  dic = {}\n",
        "  count = 0\n",
        "  for pair in preds_dict[\"start_end\"]:\n",
        "    if len(pair[0].split(\" \")) == 4:\n",
        "      start = extract_indexes_start(pair[0], predicted_tokens)\n",
        "      end = extract_indexes_end(pair[1], predicted_tokens)\n",
        "      stop = False\n",
        "      i = 0\n",
        "      if start != [] and end != [] and len(start) > 0 and len(end) > 0:\n",
        "        if len(start) >= 1:\n",
        "          while stop != True and i != len(start):\n",
        "            if start[i] not in dic.keys():\n",
        "              stop = True\n",
        "            else:\n",
        "              i+=1\n",
        "\n",
        "        if i < len(start):\n",
        "          if len(end) >= 1:\n",
        "            for j in end:\n",
        "              if j < start[i]:\n",
        "                end.remove(j)\n",
        "          dic[start[i]] = end\n",
        "      elif start == [] and end != [] and len(end) > 0:\n",
        "        start = extract_indexes_start2(pair[0], predicted_tokens)\n",
        "        if len(start) >= 1:\n",
        "          while stop != True and i != len(start):\n",
        "            if start[i] not in dic.keys():\n",
        "              stop = True\n",
        "            else:\n",
        "              i+=1\n",
        "\n",
        "        if i < len(start):\n",
        "          if len(end) >= 1:\n",
        "            for j in end:\n",
        "              if j < start[i]:\n",
        "                end.remove(j)\n",
        "          dic[start[i]] = end\n",
        "  return dic\n",
        "\n",
        "\n",
        "def get_all_sentences_indexes(preds_dict, predicted_tokens):\n",
        "  start_end = extract_start_end_indexes(preds_dict, predicted_tokens)\n",
        "  start = []\n",
        "  end = []\n",
        "  boundaries = []\n",
        "  if start_end != None:\n",
        "    if len(start_end) >= 1:\n",
        "      for key,value in start_end.items():\n",
        "        if key != [] and value != []:\n",
        "          start.append(key)\n",
        "          end.append(value[0])\n",
        "      inbetween = sorted(extract_indexes_boundaries(preds_dict, predicted_tokens))\n",
        "      for token in start:\n",
        "        if token in inbetween:\n",
        "          inbetween.remove(token)\n",
        "      for token in end:\n",
        "        if token in inbetween:\n",
        "          inbetween.remove(token)\n",
        "          \n",
        "      for token in range(len(start)):\n",
        "        tokens = [start[token],end[token]]\n",
        "        for tok in inbetween:\n",
        "          if tok > start[token] and tok < end[token]:\n",
        "            tokens.append(tok)\n",
        "        tokens = sorted(tokens)\n",
        "        for i in range(len(tokens)-1):\n",
        "          boundaries.append([tokens[i],tokens[i+1]])\n",
        "          tokens[i+1] += 1\n",
        "\n",
        "    for token in end:\n",
        "      if token not in inbetween:\n",
        "        if (\".\" in predicted_tokens[token].text or \"?\" in predicted_tokens[token].text or \"!\" in predicted_tokens[token].text or \";\" in predicted_tokens[token].text) == False:\n",
        "          for pair in boundaries:\n",
        "            if token == pair[1]:\n",
        "              boundaries.remove(pair)\n",
        "    boundaries.sort(key=lambda x: x[0])\n",
        "    \n",
        "  return boundaries\n",
        "\n",
        "\n",
        "def extract_sentences(all_indexes,predicted_tokens):\n",
        "  dic = {}\n",
        "  count = 0\n",
        "  for index in all_indexes:\n",
        "    sent = \"\"\n",
        "    words = []\n",
        "    for ind in range(index[0],index[1]+1):\n",
        "      words.append(predicted_tokens[ind].text)\n",
        "      sent += predicted_tokens[ind].text\n",
        "      sent += \" \"\n",
        "    sent = sent.rstrip()\n",
        "    dic[count] = sent\n",
        "    count += 1\n",
        "\n",
        "  return dic"
      ],
      "metadata": {
        "id": "CaHiQyOvTC00"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for NER"
      ],
      "metadata": {
        "id": "YI_yR8PUs_bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NER(all_indexes,predicted_tokens):\n",
        "  ner = spacy.load(\"en_core_web_sm\")\n",
        "  dic = {}\n",
        "\n",
        "  for index in all_indexes:\n",
        "    sent = \"\"\n",
        "    words = []\n",
        "    for ind in range(index[0],index[1]+1):\n",
        "      words.append(predicted_tokens[ind].text)\n",
        "      sent += predicted_tokens[ind].text\n",
        "      sent += \" \"\n",
        "    sent = sent.rstrip()\n",
        "    ner_results = ner(sent)\n",
        "    if len(ner_results) >= 1:\n",
        "      for word in ner_results.ents:\n",
        "        text = word.text\n",
        "        label = word.label_\n",
        "        for i in range(len(text.split(\" \"))):\n",
        "          if text.split(\" \")[i] in words:\n",
        "            index_dict = words.index(text.split(\" \")[i]) + index[0]\n",
        "            dic[index_dict] = label \n",
        "          elif text.split(\" \")[i] + \".\" in words:\n",
        "            index_dict = words.index(text.split(\" \")[i]+\".\") + index[0]\n",
        "            dic[index_dict] = label \n",
        "          elif text.split(\" \")[i] + \"?\" in words:\n",
        "            index_dict = words.index(text.split(\" \")[i]+\"?\") + index[0]\n",
        "            dic[index_dict] = label \n",
        "          elif text.split(\" \")[i] + \"!\" in words:\n",
        "            index_dict = words.index(text.split(\" \")[i]+\"!\") + index[0]\n",
        "            dic[index_dict] = label \n",
        "\n",
        "            \n",
        "\n",
        "  return dic\n",
        "\n"
      ],
      "metadata": {
        "id": "lKjjhExctK4h"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add highlights"
      ],
      "metadata": {
        "id": "isd_CJX1c2x6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if you get the error \" module 'PIL.TiffTags' has no attribute 'IFD' \" in the below cell, restart the runtime in Google Colab. Or if the run_artefact function does not return anything also restart the runtime. In this case the same errors occurs but due to the try except statement it is ignored. Note that each page takes about 20 seconds to process."
      ],
      "metadata": {
        "id": "QuIg0fbAsWeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_artefact(file_path, model_sbd):\n",
        "  ### Takes as input a file path and adds highlights in that file based on the predicted sentence boundaries.\n",
        "  ### Loops over the extracted coordinates and based on these coordinates highlights text in the file\n",
        "\n",
        "\n",
        "  ### Model for block-level DLA is pre-trained Mask-RCNN on the PubLayNet dataset\n",
        "  model_layout = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config',\n",
        "                                  extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.5],\n",
        "                                  label_map={0: \"Text\", 1: \"Title\", 2: \"List\"})\n",
        "  \n",
        "  ### Model for word-level DLA is pre-trained LayoutLM on the DocBank dataset\n",
        "  pdf_predictor = HierarchicalPDFPredictor.from_pretrained(\"allenai/hvila-block-layoutlm-finetuned-docbank\")\n",
        "  \n",
        "  ### Loops over the file page for page and extracts sentence boundary coordinates after which it highlights these.\n",
        "  pdf_file = fitz.open(file_path)\n",
        "  all_sentences = {}\n",
        "  for p in range(len(pdf_file)):\n",
        "    print(p)\n",
        "    try:\n",
        "      page = pdf_file[p]\n",
        "      #Extract text blocks and perform SBD on these text blocks\n",
        "      preds_dict, predicted_tokens = extract_boundaries(tokenizer, file_path, p, pdf_predictor, model_layout,model_sbd)\n",
        "      #Determine the index of the sentence ending tokens and beginning tokens\n",
        "      all_indexes = get_all_sentences_indexes(preds_dict, predicted_tokens)\n",
        "      #Extract all the sentences on the given page\n",
        "      sentences = extract_sentences(all_indexes,predicted_tokens)\n",
        "      #Add the extracted sentences to the dictionary containing all sentences ordered by page\n",
        "      all_sentences[p] = sentences\n",
        "      #Perform NER on the extracted sentences\n",
        "      ner = NER(all_indexes,predicted_tokens)\n",
        " \n",
        "      for i in all_indexes:\n",
        "        x0,y0,x1,y1 = extract_coordinates(predicted_tokens,i[1])\n",
        "        ul = (x0,y0)\n",
        "        ur = (x1, y0)\n",
        "        ll = (x0,y1)\n",
        "        lr = (x1,y1)\n",
        "        q = fitz.Quad(ul, ur, ll, lr)\n",
        "        highlight = page.add_highlight_annot(q)\n",
        "        highlight.set_colors(stroke=[1,1,0])\n",
        "        highlight.update()\n",
        "\n",
        "\n",
        "      for key,value in ner.items():\n",
        "        x0,y0,x1,y1 = extract_coordinates(predicted_tokens,key)\n",
        "        ul = (x0,y0)\n",
        "        ur = (x1, y0)\n",
        "        ll = (x0,y1)\n",
        "        lr = (x1,y1)\n",
        "        q = fitz.Quad(ul, ur, ll, lr)\n",
        "        highlight = page.add_highlight_annot(q)\n",
        "        if value == \"GPE\": #place\n",
        "          highlight.set_colors(stroke=[0.2,1,1]) # light blue\n",
        "          highlight.update()\n",
        "        elif value == \"DATE\":\n",
        "          highlight.set_colors(stroke=[0.2, 0.7, 0.4]) # green\n",
        "          highlight.update()\n",
        "        elif value == \"PER\":\n",
        "          highlight.set_colors(stroke=[0.3, 0, 0.7]) # purple\n",
        "          highlight.update()\n",
        "        elif value == \"ORG\":\n",
        "          highlight.set_colors(stroke=[0.7, 0.7, 0.8]) # light grey\n",
        "          highlight.update()\n",
        "        elif value == \"ORDINAL\":\n",
        "          highlight.set_colors(stroke=[0.8, 0.5, 0.0]) # light orange\n",
        "          highlight.update()\n",
        "        else:\n",
        "          highlight.set_colors(stroke=[1.0, 0.9, 1.0]) # light pink\n",
        "          highlight.update()\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  ### File is saved containing the exact same layout as before, but with added highlights\n",
        "  pdf_file.save(\"highlighted.pdf\")\n",
        "\n",
        "  return all_sentences\n",
        "    \n",
        "extracted_sentences = run_artefact(file_path,model_sbd)\n"
      ],
      "metadata": {
        "id": "j7_x5tytyPMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zFC3y-8hYqks"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}